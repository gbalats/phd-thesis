\label{chapter:related}
\epigraph{Obviously, you're not a golfer.}{\textit{The Dude}}

This chapter includes related work of previous chapters (in Sections
\ref{related:sect/structsens} -- \ref{related:sect/hiercomp})
and then considers more generic directions in static analysis
literature (in Section~\ref{related:sect/misc}).
%
Specifically,
\begin{inparaenum}[(a)]
\item Section~\ref{related:sect/structsens} contains related work for
  Chapter~\ref{chapter:structsens};
\item Section~\ref{related:sect/reflection} contains related work for
  Chapter~\ref{chapter:reflection}; and
\item Section~\ref{related:sect/hiercomp} for
  Chapter~\ref{chapter:hiercomp}.
\end{inparaenum}

\section{Field-Sensitive C/C++ Pointer Analysis}
\label{related:sect/structsens}

Regarding our structure-sensitive pointer analysis, we discussed some
closely related work throughout Chapter~\ref{chapter:structsens} (most
importantly \cite{paste/PearceKH04,toplas/PearceKH07}). More
generally, most C and C++ analyses in the past have focused on
scalability, at the expense of precision. Several (e.g.,
\cite{antgrasshopper,popl/ZhengR08,popl/Lhotak11}) do not model more
than a small fraction of the functionality of modern intermediate
languages.

One important addition is the DSA work of Lattner et
al.~\cite{pldi/LattnerLA07}, which was the original points-to analysis
in LLVM. The analysis is no longer maintained, so comparing
experimentally is not possible. In terms of a qualitative comparison,
the DSA analysis is a sophisticated but ad hoc mix of techniques, some
of which add precision, while others sacrifice it for scalability. For
instance, the analysis is field-sensitive using byte offsets, at both
the source and the target of points-to edges. However, when a single
abstract object is found to be used with two different types, the
analysis reverts to collapsing all its fields. (Our analysis would
instead create two abstract objects for the two different types.)
Furthermore, the DSA analysis is unification-based (a Steensgaard
analysis), keeping coarser abstract object sets and points-to sets
than our inclusion-based analysis. Finally, the DSA analysis uses deep
context-sensitivity, yet discards it inside a strongly connected
component of methods.
%Overall, the DSA
%analysis is intriguing and it would be interesting to compare its
%precision with our approach, 

The field-sensitive inclusion-based analysis of Avots et
al. \cite{icse/AvotsDLL05} uses type information to improve its
precision. As in this work, they explicitly track the types of objects
and their fields, and filter out field accesses whose base object has
an incompatible type (which may arise due to analysis
imprecision). However, their approach is array-insensitive and does
not employ any kind of type back-propagation to create more
(fine-grained) abstract objects for polymorphic allocation
sites. Instead, they consider objects used with multiple types as
possible type violations. Finally, they extend type compatibility with
a form of structural equivalence to mark types with identical physical
layouts as compatible. The implementation of \cclyzer{} applies a more
general form of type compatibility, presented in
Section~\ref{structsens/sect/addons}.

\citeauthor{lctrts/Mine06} \cite{lctrts/Mine06} presents a highly
precise analysis, expressed in the abstract interpretation framework,
that translates any field and array accesses to pointer arithmetic. By
relying on an external numerical interval analysis, this technique is
able to handle arbitrary integer computations, and, thus, any kind of
pointer arithmetic. However, the precision comes with scalability and
applicability limitations: the technique can only analyze programs
without dynamic memory allocation or recursion.
%(which commonly applies only to embedded critical software).

There are similarly other C/C++-based analyses that claim
field sensitivity~\cite{popl/HardekopfL09,cgo/HardekopfL11}, but it is
unclear at what granularity this is implemented. Existing descriptions
in the literature do not match the precision of our
structure-sensitive approach, which maintains maximal structure
information (with typed abstract objects and full distinction of subobjects), at
both sources and targets of points-to relationships. Nystrom et
al.~\cite{paste/NystromKH04} have a fine-grained heap abstraction that
corresponds to standard use of ``heap cloning''
(a.k.a. ``context-sensitive heap'').% techniques.


\section{Static Analysis and Reflection}
\label{related:sect/reflection}

The traditional handling of reflection in static analysis has been
through integration of user input or dynamic information.  The
Tamiflex tool~\cite{icse/BoddenSSOM11} exemplifies the state of the
art. The tool observes the reflective calls in an actual execution of
the program and rewrites the original code to produce a version
without reflection calls. Instead, all original reflection calls
become calls that perform identically to the observed execution. This
is a practical approach, but results in a blend of dynamic and static
analysis.
% Clearly, the greatest motivation for
%static analysis is to capture all possible program behaviors.
It is unrealistic to expect that uses of reflection will always yield
the same results in different dynamic executions---or there would be
little reason to have the reflection (as opposed to static code) in
the first place. Our approach attempts to restore the benefits of
static analysis, with reasonable empirical soundness.

An alternative approach is that of Hirzel et
al.~\cite{ecoop/HirzelDH04,toplas/HirzelDDH07}, where an online
pointer analysis is used to deal with reflection and dynamic loading
by monitoring their run-time occurrence, recording their results, and
running the analysis again, incrementally. This approach is quite
interesting when applicable. However, it is not applicable in many
static analysis settings. Maintaining and running a precise static
analysis during program run time is often not realistic (e.g., for
expensive context-sensitive analyses). Furthermore, the approach does
not offer the off-line soundness guarantees one may expect from static
analysis: it is not possible to ask questions regarding all methods
that may ever be called via reflection, only the ones that have been
called so far.

Interesting work on static treatments of reflection is often in the
context of dynamic languages, where resolving reflective invocations
is a necessity.  Furr et al.~\cite{oopsla/FurrAF09} offer an analysis
of how dynamic features are used in the Ruby language. Their
observations are similar to ours: dynamic features (reflection in our
case) are often used either with sets of constant arguments (in order
to avoid writing verbose, formulaic code), or with known
prefixes/suffixes (e.g., to re-locate within the file system).

Madsen et al.~\cite{sigsoft/MadsenLF13} employ a use-based analysis
technique in the context of Javascript. When objects are retrieved
from unknown code (typically libraries) the analysis infers the
object's properties from the way it is used in the client. In
principle, this is a similar approach to our use-based techniques of
Section~\ref{reflection/sec:use-based} (both object invention and
back-propagation) although the technical specifics differ. The
conceptual precursor to both approaches is the work on reflection by
Livshits et al.~\cite{aplas/LivshitsWL05,livshits:thesis}, which has
been extensively discussed and contrasted throughout
Chapter~\ref{chapter:reflection} (see
Sections~\ref{reflection/sec:model}, and
\ref{reflection/sec:use-based}).

Advanced techniques for string analysis have been presented by
Christensen et al.~\cite{sas/ChristensenMS03}. They analyze complex
string expressions and abstract them via a context-free grammar that
is then widened to a regular language. The regular approximations
produced by this approach are richer than the prefix and suffix
matching of our substring analysis, and can thus better approximate
the possible values of arbitrary string expressions. Reflection is one
of their examples but they only apply it to small benchmarks.

\citeauthor{ecoop/AliL12}~\cite{ecoop/AliL12,ecoop/AliL13} offer
comparisons of dynamic and static call-graph edge metrics.  They
discover hundreds of missing edges in several of the DaCapo
2006-10-MR2 benchmarks.  However, their experiments do not integrate
the vastly improved support for reflection (e.g., modeling of
\javasignature{Object.getClass}) offered by \textsc{Elf} or our
current work.

%were performed
%before numerous engineering improvements in the \textsc{Doop}
%framework.
%These improvements handle many common API calls, such as
%\javasignature{Object.getClass}.
%Furthermore, some benchmarks could not be run with reflection enabled
%due to scalability issues. Therefore, o
%%SPACE
%Our experiments are substantially more representative in regards to
%the actual empirical soundness of a joint reflection and pointer
%analysis.
Stancu et al.~\cite{pppj/StancuWBLF14} empirically
%present an empirical study that
compare profiling data with a points-to static analysis.
%,but do not support reflection.
However, they target only the most reflection-light benchmarks of the
DaCapo 9.12-Bach suite (\emph{avrora, luindex, and lusearch}), and
patch the code to avoid reflection entirely.

% TODO consider adding, after move
%Some of the most closely related work to ours is the Li et
%al. work~\cite{ecoop/LiTSX14}, discussed earlier. Li et al. also
%improve on the engineering aspects of reflection handling in the
%\textsc{Doop} framework, as well as employ a back-propagation
%technique. The work is concurrent to ours and we have only been able
%to obtain a preprint at the time of this writing. Nevertheless, the
%differences of the two approaches seem quite clear, as also discussed
%in Section~\ref{reflection/sec:back-propagation}. Furthermore, the Li et al.
%technique does not employ substring matching or substring flow
%analysis and does not emphasize empirical soundness. Instead it
%eagerly under-approximates many more reflective calls and resolves
%such calls only when forward and backward reflection analysis
%information agrees. A valuable aspect of the Li et al. work is a
%detailed study of how reflection is used in practical programs.


\section{Class Hierarchy Complementation and Static Analysis on
  Partial Programs}
\label{related:sect/hiercomp}

The hierarchy complementation problem, which we presented in
Chapter~\ref{chapter:hiercomp}, is in principle new, although
indirectly related to various other pieces of work in the literature.

From a theory standpoint, our problem is an attempt to more fully
determine the structure of a partially ordered set. There is no exact
counterpart of our algorithms in the literature. However, there has
been work on sorting a poset, i.e., completely determining the partial
order \cite{soda/DaskalakisKMRV09}.  The challenge in such algorithmic
research, however, is to perform the sorting with a minimal number of
queries. None of the interesting devices of our algorithms are
present. Specifically, the device of the single inheritance case (if a
node can reach two others, they have to be ordered relative to each
other) does not apply, and neither does the interesting constraint of
the multiple inheritance case (we cannot add direct supertypes to a
known node).

Complementing a program so that the result respects static properties
is analogous to analyzing only parts of a program but giving
guarantees on the result. There are few examples of program analyses
of this nature. Notably, \citeauthor{ecoop/AliL12} introduce
a technique \cite{ecoop/AliL12} for analyzing an application
separately from a library, while keeping enough information (from the
library analysis) to guarantee that the application-level call-graph
is correct. Furthermore, the Averroes system \cite{ecoop/AliL13} uses
the assumption that the missing code is independently developed in
order to produce a worst-case skeletal library. That is, Averroes
takes an existing library and strips away the implementation, keeping
only the interface between the library and the application. The
implementation is replaced with code (at the bytecode level) that
performs worst-case actions on the arguments passed into the library,
for the purposes of call-graph construction (i.e., the generated code
calls all methods the eliminated original code could ever
call). Averroes is related to JPhantom but at rather opposite ends of
the spectrum: Averroes produces worst-case skeletal implementations,
while JPhantom produces minimal, best-case implementations that still
respect well-formedness at the type level.  At the same time, Averroes
assumes the library interface is available and just tries to avoid
analyzing library implementations, while JPhantom applies precisely
when the library is completely missing. Thus, JPhantom truly applies
to the case of partial programs, whereas Averroes analyzes a partial
program but under the assumption that the whole program was available
to begin with. It would be interesting to treat a partial program
first with JPhantom and then apply Averroes to the JPhantom-produced
program complement to obtain the worst-case behavior of a plausible
interface for the missing code.

Analyzing partial programs at the level of source code has been
studied by \citeauthor{oopsla/DagenaisH08}
\cite{oopsla/DagenaisH08}. Apart from the elaborate type constraints
that may arise from missing source code, as discussed in
Section~\ref{hiercomp/sect/discussion}, another important challenge is
syntactic ambiguities (e.g., \emph{is it a package or a class
  name?}). The goal of \citeauthor{oopsla/DagenaisH08} is to be able
to deal with both typing problems and syntactic ambiguities in any
given partial Java program and produce a typed IR (although it may
contain placeholders representing unknown types and packages). Their
approach is primarily based on heuristics that make arbitrary choices
on various occasions (with no form of backtracking) that may
eventually lead to wrong results. Even though they, too, identify
similar subtype constraints and record missing class members to be
added in the typed IR, they do not define any similar type hierarchy
complementation problem or come up with algorithms to systematically
construct a valid solution in such a setting.

% - deal with syntactic ambiguities as well as typing problems
% - builds a typed IR (typed AST and Jimple) from a partial program with
%   no unknown syntactic constructs
% - share subtype constraints and adding missing class members
% - no backtracking, heuristic-based approach
% - heuristics make arbitrary choices that may produce wrong results
% - may yield unknown type and package placeholders

Our hierarchy complementation problem bears a superficial resemblance
to the \emph{principal typings} problem
\cite{popl/AnconaZ04,popl/AnconaDDZ05,Ancona04even}.  The principal
typings problem consists of computing types for a Java module in
complete isolation from every other module it references. I.e.,
principal typings aim to achieve a more aggressive form of separate
compilation, by computing the minimal type information on other
classes that a class needs in order to typecheck and compile. Thus,
the motivation is fairly similar to ours, but the technical problem is
quite different.  First, in our setting we already have the result of
compilation in the form of bytecode, and bytecode only. Second, our
emphasis is on satisfying constraints instead of capturing them as a
rich type. Finally, our constraints are of a very different nature
from any in the principal typings literature. As discussed in
Section~\ref{hiercomp/sect/discussion}, the input and output language
assumptions crucially determine the essence of an incomplete program
analysis problem.



\section{Other Directions in Static Analysis}
\label{related:sect/misc}

In this section, we will extend our focus and examine more generic
directions and different methodologies in static analysis literature.


%% -----------------------------------------------------------------------------
%   CFL Reachability Problems
%% -----------------------------------------------------------------------------

\paragraph{CFL reachability formulation.} In Chapters
\ref{chapter:structsens} and \ref{chapter:reflection}, we formulated
pointer analysis algorithms in inference rules that can be
straightforwardly expressed in Datalog. Employing a restricted
language not only offers guarantees of termination and complexity
bounds, but also permits more aggressive optimization of the language
features.

Along these lines, pointer analysis and other related analyses have
been formulated as a \emph{context-free language (CFL) reachability}
problem. The idea is that we may encode an input program as a labeled
graph, and a specific analysis corresponds to the definition of a
context-free grammar, \(G\). The relation being computed holds for two
nodes of the graph if and only if there exists a path from one node to
the other, such that the concatenation of the labels of edges along
the path belongs in the language \(L(G)\) defined by \(G\).

Specifically, the input graph normally consists of nodes representing
program elements such as variables, types, methods, statements, and so
on. Edges represent relations between those program elements. For
instance, an edge \((s,\,t)\) may represent that there exists an
assignment from variable \(s\) to variable \(t\). Moreover, edges may
encode field accesses (load/store), method invocations, pointer
dereferences, etc, and hence may even connect different \emph{kinds}
of program elements. The exact choice of domains depends on the
specific analysis being run and the problem it addresses. Since we
want to express many input relations, we need many types of edges,
represented as labels (e.g., we can label a field access edge by some
symbol denoting field access plus the name of the field).  For a given
analysis, a context-free grammar \(G\) encodes the desired computed
relations (e.g., which pointers are memory aliases) as non-terminal
symbols, and supplies production rules that express how they relate to
the simpler relations represented by graph edges (terminals). The CFL
reachability answer is then commonly computed by employing a dynamic
programming algorithm.

The first application of such a framework in program analysis was
designed to solve various interprocedural dataflow-analysis problems
\cite{popl/RepsHS95}, but CFL reachability has since been used in a
wide range of problems, such as:
\begin{inparaenum}[(i)]
\item the computation of points-to relations
  \cite{journals/infsof/Reps98},
\item the (demand-driven) computation of may-alias pairs for a C-like
  language \cite{popl/ZhengR08},
\item Andersen-style pointer analysis for Java
  \cite{oopsla/SridharanGSB05}.
\end{inparaenum}

Any CFL reachability problem can be converted to a Datalog program
\cite{journals/infsof/Reps98}, but the inverse is not true (i.e., CFL
reachability corresponds to a restricted class of Datalog programs,
the so-called ``chain programs''). Thus, the primary advantage of CFL
reachability is that it permits more efficient implementations.

A \emph{chain program} consists of rules of the form:

\[p(X, Y) \leftarrow q_0(X,Z_1),\, q_1(Z_1, Z_2),\, \dots,\, q_k(Z_k,
  Y). \]

We can express a CFL reachability problem in Datalog by using such a
chain rule to represent the following production of grammar \(G\):

\[p \rightarrow q_0\; q_1\; \dots\; q_k \]

where a Datalog fact \(e(m, n)\) represents an edge \((m, n)\) labeled
\(e\) in the graph.

An even more restrictive variant, \emph{Dyck-CFL reachability}, can be
obtained by restricting the underlying CFL to a Dyck language, i.e.,
one that generates balanced-parentheses expressions. Although
restrictive, this approach still suffices for some simple pointer
analysis algorithms, while allowing very aggressive optimization,
often with impressive performance gains \cite{pldi/ZhangLYS13}.

% -CFL formulations (Dyck-CFL PLDI'13, Rugina, Bodik)
% \citep{pldi/ZhangLYS13}             % fast Dyck-CFL
% \citep{popl/ZhengR08}               % demand-driven; may-alias pairs
% \citep{journals/infsof/Reps98}      % also published in SLP' 1997; points-to relations
% \citep{popl/RepsHS95}               % interprocedural dataflow-analysis problems
% \citep{cc/Reps94}                   % magic-sets transformation -> demand-driven
% \citep{oopsla/SridharanGSB05}       % Java; demand-driven; balanced parentheses (field accesses)



%% -----------------------------------------------------------------------------
%   Shape Analysis
%% -----------------------------------------------------------------------------

\paragraph{Shape Analysis.}

So far, the techniques we have presented for pointer analysis have
been based on the allocation sites as the primary source for inventing
new abstract objects in memory. Despite our deviation from the
standard allocation-site abstraction, where a single abstract object
will be allocated per allocation site, even our own techniques
described in Chapter~\ref{chapter:structsens} will use the allocation
site as the basis of abstraction (but may create multiple objects per
site, nonetheless). A different approach altogether is that of the
techniques termed as \emph{shape analysis}
\cite{toplas/SagivRW02,popl/SagivRW99,sas/ManevichSRF04,sas/Lev-AmiS00,toplas/SagivRW98,sefm/FerraraFJ12}.
The primary goal of standard shape analysis is to be able to infer the
\emph{shapes} of objects in memory. E.g., to be able to detect if some
objects form a list or a tree, if some list may contain cycles, if a
subtree or a portion of a list may be shared (i.e., be reachable from
multiple objects), and so on.\footnote{The term shape analysis is
  quite generic (i.e., any analysis designed to infer the shapes of
  objects) and has been examined in many different contexts
  \cite{popl/GhiyaH96}. Here, we focus on \emph{parametric shape
    analysis via 3-valued logic}, as one of the most notable
  methodologies in that area.}

To achieve such a feat, shape analysis associates a list of properties
to each abstract memory object (e.g., \emph{pointed by variable
  \var{v}}, \emph{transitively reachable by variable \var{r}}, and so
on) and uses Kleene's three-valued logic to differentiate between must
and may information. For instance, if the ``\emph{pointed by variable}
\(\var{v}\)'' property of an abstract object \(\alloc{obj}\) has the
value 1 at some memory state, it means that variable \var{v}
\emph{must} point to this object (at the given state), whereas the
value \(1\over{2}\) would represent our familiar \emph{may} notion of
points-to. Hence, shape analysis performs an amalgam of must and may
analysis simultaneously.

At each memory state the analysis has computed, it tries to collapse
\(1\over{2}\) values of properties to either 0 or 1, via the so called
\emph{focus operation}. Inconsistent states are then discarded at the
\emph{coerce operation}. Thus, the analysis dynamically tries to
eliminate uncertainty by \emph{focusing} on the values of some core
predicates (and statement-specific formulas), at the expense of
possible memory state explosion---the abstract interpretation of each
program statement tends to create multiple output states for each one
of its input states.  As for abstract objects, they are defined only
by the values they have for some basic properties (called
\emph{abstraction properties}) of the particular analysis. Therefore,
the upper bound for the number of abstract objects in memory is
exponential with respect to the number of abstraction predicates
defined. The latter will almost certainly include a predicate per each
program variable.

By applying these techniques, and choosing the right abstraction
predicates, the analysis will be able to carve out the \emph{shapes}
of objects in memory. For instance, given an input memory state where
variable \var{v} points to the head of some list \(l\) with at least
two elements, the abstract interpretation of instruction ``\code{v =
  v->next}'' will create multiple output memory states where either:
\begin{inparaenum}[(i)]
\item \var{v} points to a new head of a list with a non-empty tail
  (corresponding to the case where \(l\) contained at least three
  elements), or
\item \var{v} points to the single element of a list with no tail at
  all (corresponding to the case where \(l\) contained exactly two
  elements).
\end{inparaenum}
In both output memory states, the analysis will compute values of 1
for the predicate ``\emph{pointed by variable \var{v}}''; hence, it
will know exactly where \var{v} points to and not lose any precision,
at the expense of increasing the possible memory states by 1.

\citeauthor{popl/SagivRW99} initiated the field of parametric shape
analysis via 3-valued logic
\cite{toplas/SagivRW98,popl/SagivRW99,toplas/SagivRW02}, and
\citeauthor{sas/Lev-AmiS00} presented the TVLA framework for shape
analysis \cite{sas/Lev-AmiS00}. Since then, there has been work on
various extensions such as
\begin{inparablank}
\item a more economic heap abstraction \cite{sas/ManevichSRF04},
\item better support for recursive programs \cite{cc/RinetzkyS01}
\item and programs with highly nested data structures
  \cite{cav/BerdineCCDOWY07} , and
\item the incorporation of a value analysis into the shape analysis
  algorithm~\cite{sefm/FerraraFJ12},
\end{inparablank}
among others.


%% -----------------------------------------------------------------------------
%   Separation Logic
%% -----------------------------------------------------------------------------

\paragraph{Separation Logic.}
Points-to analysis provides a model of the heap (or of memory in
general, for a language such as C). Other approaches for heap analysis
that can be used to prove pointer safety are based on the field of
\emph{separation logic}. Separation logic, in turn, can be viewed as
an extension of Hoare logic
\cite{journals/cacm/Hoare69,floyd1967assigning,lics:2002/Reynolds,csl/OHearnRY01}.
Hoare logic is a formal system for reasoning about the correctness of
programs, by encoding the programming language's semantics in
\emph{Hoare tiples}. A Hoare triple has the form \(\{P\}\, C\, \{Q\}\)
and describes how the execution of a command changes the state of the
computation. Specifically, it states that whenever the assertion \(P\)
holds, before executing command \(C\), then assertion \(Q\) will hold
afterwards (if \(C\) terminates). The \(P,\, Q\) assertions can
express conditions on program variables, written by using standard
mathematical notations together with logical operators (or, in
general, some form of calculus like \emph{first-order logic}).

Hoare logic provides two ways to generate verification conditions:
\begin{inparaenum}[(i)]
\item either forwards, by starting from a precondition and
  generating formulas to prove a postcondition
\item or backwards, by starting from a postcondition and trying to
  prove a precondition.
\end{inparaenum}
Either way, in the general case, it cannot provide fully automated
reasoning; building a proof may require human guidance.

% Even though there has been effort in increasing the level of
% automation (e.g., for the (semi-)automatic inference of loop
% invariants)

Separation logic
\cite{lics:2002/Reynolds,csl/OHearnRY01,popl/IshtiaqO01,Reynolds00intuitionisticreasoning}
extends Hoare logic by introducing additional operators in the syntax
of assertions, that facilitate local reasoning. Namely, the
\emph{separating conjunction} \(\,P * \,Q\) asserts that \(P\) and
\(Q\) hold for separate portions of memory, and thus can be used on
program-proof rules to provide modular reasoning about
programs. Additional operators include the separating implication,
\(P \sep Q\), which asserts that if the current heap is extended with
a disjoint part in which \(P\) holds, then \(Q\) will hold in the
extended heap, and more. Note that, these operators do not increase
the ``completeness'' of Hoare logic---what can be proven in separation
logic can also be proven in Hoare logic. Rather, they merely simplify
the specifications and proofs.

% citation needed
% Peter O'Hearn
% A Primer on Separation Logic (and Automatic Program Verification and Analysis)

\citeauthor{popl/CalcagnoDOY09} present a compositional shape analysis
\cite{popl/CalcagnoDOY09} to be used in (lightweight) program
verification that builds on these concepts of separation logic
(instead of the TVLA approach of \citeauthor{popl/SagivRW99}). In
classical logic, \emph{abduction} stands for the inference of
``missing'' assumptions \(M\), such that, given another assumption
\(A\) and a goal \(G\), one can prove \(G\) by synthesizing \(A\) and
\(M\):
\[A \,\land M\, \;\vdash\; G \]

A similar problem can be phrased by using separating conjunction
instead of classical conjunction, which also partitions the premises:
\[A \;*\; M\, \;\vdash\; G \]

This finally leads to the more general problem of \emph{bi-abduction},
if we allow for leftover portions of state (the frame):
\[A \,*\, \textit{?anti-frame} \;\vdash\; G \,*\, \textit{?frame} \]

The notion of bi-abduction is used as the basis of a new compositional
interprocedural shape analysis algorithm.


\paragraph{Abstraction Strategies.} In Chapter~\ref{chapter:structsens}, we
used an abstraction policy for memory objects that may produce more
than one abstract objects per allocation site. The exact strategy by
which memory objects are represented by a static analysis is a crucial
design choice.

Using the allocation site as a means of abstraction dates at least
back to the work of \citeauthor{popl/JonesM79} \cite{popl/JonesM79},
who use graphs to summarize heap data structures. An allocation site
(i.e., cons instruction) is handled by creating a new graph node that
may represent multiple concrete allocations for a given execution. The
same principle for abstracting cons cells is used later in the work of
\citeauthor*{pldi/ChaseWZ90} \cite{pldi/ChaseWZ90}.

Previously, we also talked about shape analysis
\cite{toplas/SagivRW02,popl/SagivRW99,toplas/SagivRW98} as another
means of abstracting memory that goes far beyond allocation sites, by
introducing the concept of arbitrary abstraction predicates. The
approach of \citeauthor{sas/BalakrishnanR06}
\cite{sas/BalakrishnanR06} lies somewhere in between. For each
allocation site, their analysis creates two abstract objects:
\begin{inparaenum}[(i)]
\item the first one represents the \emph{most-recently-allocated}
  object (for the given site), while
\item the second one summarizes all the other (least recent)
  objects for the same site.
\end{inparaenum}
Note that the first object abstracts just a \emph{single} concrete
object and, hence, can be used to perform strong updates, which is an
essential technique to improve the precision and scalability of a
flow-sensitive analysis.
%
In contrast, shape analysis would instead record a summary abstraction
predicate for each abstract object that captures whether it represents
more than one concrete objects. This would naturally lead to the
specialization of non-summary nodes that exhibit interesting shape
properties (such as the head of some list structure) according to the
rest of the abstraction criteria.
%
\citeauthor{journals/corr/KanvarK14} present a thorough taxonomy of
various other strategies in heap abstractions
\cite{journals/corr/KanvarK14}.


% ------------------------------------------------------------------------------
%  Context Sensitivity
% ------------------------------------------------------------------------------

\paragraph{Context Sensitivity.}

A pointer analysis attempts to compute an abstract yet reasonably
precise model of the program's memory for all possible
executions. Recall the basic query that such a model needs to answer,
from Chapter~\ref{chapter:structsens}:
\begin{displayquote}
  What can a pointer variable point to, i.e., what can its value be
  when dereferenced during program execution?
\end{displayquote}
Of course, if the same question is phrased in the Java setting---in
Java, variable references point to heap allocations---instead of
C/C++, it becomes:
\begin{displayquote}
  What heap objects can each reference variable point to, during
  program execution?
\end{displayquote}

As discussed in the previous paragraph, there are many different
approaches on how to abstract memory objects. Simpler approaches, like
the allocation-site abstraction, lead to more efficiently computable
memory models, whereas on the opposite end we can have models with
complex abstractions, such as shape analysis, that do not scale well
with program size.

A generic approach that increases the granularity of the analysis
abstractions and yields greater precision is that of context
sensitivity. The idea is to qualify variables and abstract objects
with context information. Thus, a points-to edge
\((v, c_1,\, \alloc{o}, c_2)\) now captures that variable \(v\), under
context \(c_1\), points to abstract object \(\alloc{o}\), allocated
under context \(c_2\). Essentially, this means that we have refined
our abstractions: the same variable \(v\) is represented by multiple
variable-context tuples, \((v, c_1)\), each one corresponding to some
different execution, and having an independent points-to
set. Similarly, the same allocation site (given that we have used the
allocation-site abstraction as a basis for our object abstraction) of
\(\alloc{o}\) is now represented by multiple object-context tuples,
\((\alloc{o}, c_2)\), each one corresponding to a different execution
context under which the allocation was made.

There are many flavors of context sensitivity in existing literature,
which use different strategies for creating contexts. To name a few:
\begin{itemize}[--]
\item \emph{Call-site sensitivity} (or \emph{k}-CFA in the context of
  functional languages) that uses method call sites as context
  elements \cite{Sharir:Interprocedural,shivers:thesis}
\item \emph{Object sensitivity}, where object allocation sites are used as
  context elements \cite{issta/MilanovaRR02,tosem/MilanovaRR05}
\item \emph{Type sensitivity}, which is similar to object sensitivity but
  instead collapses the objects to their types to achieve better
  scalability \cite{popl/SmaragdakisBL11}
\item \emph{Hybrid context-sensitivity} that selectively combines
  call-site and object sensitivity \cite{pldi/KastrinisS13}.
\end{itemize}
Each approach has its relative merits and unique performance
characteristics; none is clearly superior to all others. The exact
choice of context has to consider the specific characteristics of the
program being analyzed. Generally, it is a matter of experimentation
but automated machine-learning techniques can also be used to adapt
the degree of context sensitivity
\cite{oopsla/OhYY15,pldi/LiangN11,popl/LiangTN11,popl/GrigoreY16}.
%
Other approaches use counter-example-guided abstraction refinements,
using the feedback from failed analysis runs to pick a new abstraction
which is less likely to fail
\cite{pldi/ZhangNY13,pldi/ZhangMGNY14,tacas/GulavaniR06}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
