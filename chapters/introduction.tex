Static program analysis is a vast field with broad uses; an umbrella
term for many different methodologies (%
\begin{inparablank}
  \item Hoare logic
    \cite{journals/cacm/Hoare69,floyd1967assigning,lics:2002/Reynolds,csl/OHearnRY01},
  \item model checking
    \cite{icalp/EmersonC80,lop/ClarkeE81,toplas/ClarkeES86,programm/QueilleS82},
  \item symbolic execution
    \cite{journals/cacm/King76,journals/tse/Howden77,conf/kbse/PasareanuR10,Boyer:1975:SFS:390016.808445},
  \item abstract interpretation
    \cite{popl/CousotC77,journals/jlp/CousotC92,journals/logcom/CousotC92},
  \item data-flow analysis
    \cite{popl/Kildall73,books/daglib/0030999,books/mk/Muchnick1997,journals/acta/KamU77,popl/RepsHS95,books/ph/SharirP81},
\end{inparablank}
and so on), that aim to automatically obtain an
understanding of a program's behavior, without running it. Nowadays,
one form or another of static analysis can be found in many different
contexts: compilers, IDEs, editors, linters, or even dedicated static
analysis tools or frameworks. The ends of a static analysis tool are
equally diversified, ranging from bug finding and program
verification to optimization, or even aided program comprehension.

Along with static analysis tools, the programming languages have
evolved as well, becoming more high-level throughout the years,
introducing many layers of abstraction, before eventually translating
the program to the machine's native opcodes. High-level languages are
appealing because they are easier to program in, and maintain. Less
programming effort (e.g., in terms of lines of code) is needed to
express some computation. Virtual machines have even abstracted away
the platform where the code will run. Instead, programs of managed
languages are translated to machine code for some \emph{virtual
  machine}, and hence may run on any platform that provides a backend
that emulates this virtual machine.

Software has evolved too. Complex design patterns, immense libraries,
frameworks implementing inversion of control, over-involved build
tools, and many other complicacies pose significant challenges to
program understanding.

As one would expect, static analyses have struggled to keep up with
the ever-increasing complexity of software and the programming
languages it is written in; the very task of automated program
understanding has become daunting, yet even more valuable.

The most promising and powerful of existing static analysis techniques
rely on the creation of some \emph{abstract memory model} of the
program. What objects will the memory contain, at some state of
execution? What will their structure be like?  A faithful abstract
representation of the actual memory is, however, a demanding task; its
precision often detrimental to the value of whatever the static
analysis is aiming to eventually compute (be it the identification of
complex bug patterns or the opportunities for effective
optimizations).

In this dissertation we argue that there is \emph{implicit structural
  information} in the program, about the memory it will allocate, that
can improve the quality of the abstract memory model constructed by
static analysis. This structural information is not readily available,
but may be recovered via inference, primarily by tracking the use of
types in the program.

We provide a number of techniques that recover such
lost memory structure, in two different settings:%
\begin{inparaenum}[(1)]
\item in C/C++ programs, as a typical case of low-level code with
  direct memory access, where the program's memory structure is often
  lost due to specific programming idioms and the inherent low-level
  nature of the language, and
\item in Java programs, where despite the high-level nature of the
  language, structural information may be lost for \emph{partial
    programs} (i.e., libraries or any programs that lack some of their
  parts), which, in the form of Java Archives (JARs), constitute the
  main distributable code entity of this managed language.
\end{inparaenum}


\section{Impact}

In this section, we will briefly explain the main contributions of
this dissertation, from both a scientific and a practical perspective.

\subsection{Scientific Contributions}

A weakly-typed language, such as C or C++, exposes pointers as numeric
values and allows the programmer to perform arbitrary arithmetic on
them. These \emph{pointer arithmetic} capabilities can be used to
bypass the language's type system. Objects may be allocated on memory
without any local information about their intended type, at the
allocation site. In fact, the norm for most heap-allocating routines
(e.g., \code{malloc()}) is to return just an array of bytes. These
allocations, while in this \emph{untyped state}, flow to various other
instructions and may be even stored to type-agnostic raw pointer
variables. Normally, when such an allocation was intended to create an
object of type \(T\), a cast instruction or an implicit conversion
will be used prior to any other instruction that expected an object of
this specific type.

\emph{Pointer analysis} is a static program analysis that determines
the values of pointer variables or expressions. For each pointer, it
computes a set of memory allocations that the pointer may point to. We
refer to this as the \emph{points-to set} of a variable. Since
computing an exact model of memory is undecidable, a static analysis
needs to sacrifice performance for computability. Thus, the memory
allocations of pointer analysis are mere abstractions; a single
allocation may represent many concrete objects at some program
execution. One such popular abstraction represents memory objects by
their allocation sites. Hence, any number of concrete objects
allocated at the same instruction correspond to a single abstract
object.

In the case of C/C++ programs, the first scientific contribution is a
\emph{revised abstract memory model} that differs from the classic
allocation-site abstraction approach, by introducing many more
abstract objects (not just one per allocation site). Such a finer
granularity, in terms of memory abstraction, is a key step for the
analysis to maintain strong type information for its abstract
objects. After all, the same allocation site can be used in C to
create allocations of more than one distinct types. Also, as will be
explained later, C allows a pointer variable to point to some field of
an allocated object. We tackle this by representing fields and array
indices of abstract objects as separate abstract (sub-)objects with
their own points-to sets. Hence, the pointer analysis can
differentiate between pointing to some abstract object, and pointing
to one of its fields (or array indices). This is commonly known as
\emph{field-sensitivity}. Due to C's expos\'{e} of pointers, a
field-sensitive pointer analysis is much harder to implement, than in
a language that doesn't provide direct memory access (e.g., Java). Our
revised memory model aims to extend the domain of abstract objects to
naturally express field sensitivity for C and C++.

The second scientific contribution in the C/C++ setting of this work
is a technique to enhance pointer analysis precision by
``dynamically'' associating and maintaining type information for all
abstract objects in memory.  By the term ``dynamically'', we mean that
any object-type association is performed simultaneously with the
pointer analysis itself (in a manner similar to \emph{on-the-fly}
call-graph construction). The pointer analysis uses the inferred types
of abstract objects to produce new points-to facts or filter spurious
inferences due to type incompatibility. The points-to sets, on the
other hand, drive the creation of new object-type associations that
may again alter the produced points-to sets, and so on---all partaking
in an interdependent recursive cycle of computation.

We use this technique to collect type hints---indications that some
abstract object has type \(T\)---and for each type discovered we
(dynamically again) create a new (typed) variant of the original
abstract object. Thus, the same allocation site may produce multiple
abstract objects for different types, while those types will be
determined on-the-fly (i.e., through the pointer analysis itself). The
plethora of abstract objects generated by this technique is in line
with the fine-grained property of our revised abstract memory model.

As an example of a type hint, which demonstrates how these two
techniques interact, consider a field access \code{((P*) p)->f}. Due
to analysis imprecision, the analysis may be unable to reason about
the type of the abstract object(s) that \code{p} points to (as it
could have been allocated via a generic \code{malloc()} call with no
type indication). Or, it may even have computed that \code{p} points
to objects of incompatible type (that do not contain any \code{f}
field, whatsoever). However, given the present static type
information, the analysis will mark \code{P} as one of the possible
types of the base abstract object \(\alloc{obj}\) (for any
\(\alloc{obj}\) that \code{p} may point to), if the type of the latter
is yet unknown. Other objects with known yet incompatible types will
be filtered out. Thus, the analysis will create a new \emph{typed}
abstract object \(\alloc{obj}_P\) of type \code{P}, which will also
flow to the points-to set of variable \code{p}. This object will now
be eligible as the base address for accessing field \code{f} (type
compatibility is guaranteed by the compiler). Finally, the analysis
will compute that the expression \code{p->f} points to the (typed)
abstract subobject that represents field \code{f} of
\(\alloc{obj}_P\). Hence, at field accesses the analysis will always
be able to recover potentially lost structural information.

In the realm of Java, the challenges are quite different. Java is a
statically and strongly typed language that doesn't expose
pointers. All objects (except that of primitive types) are allocated
in the heap, and accessed via references (allocated on the
stack). References are the disciplined equivalent of a C pointer, and
allow no pointer arithmetic at all. All heap allocations of Java have
a single (dynamic) type, declared at the allocation site. Objects of
composite types can only contain references to other objects and there
is no way to store a reference to an object's field. Hence, pointer
analysis can be expressed via a much simpler memory model, based on the
allocation-site abstraction.

However, Java has another crucial difference from C/C++: it is a
managed language. All Java code is translated to a
platform-independent IR, which is Java bytecode, to be executed by a
Java Virtual Machine (JVM). Using just-in-time (JIT) compilation, the
JVM will translate the bytecode to machine code---more precisely, the
JVM will jit-compile some parts of the bytecode, specifically, the
most frequently called methods or methods with long-running loops
(also known as \emph{hot spots}), and interpret the rest of it
\cite{journals/taco/KotzmannWMRRC08}.

Java also introduces the concept of a Java Archive (JAR), which is a
bundle of class files (compilation units in bytecode format), and
possibly other files as well, using a ZIP file format. Since JARs
contain essentially bytecode, they are platform-independent as
well. Build tools, such as Apache Maven \cite{www:maven}, Gradle
\cite{www:gradle}, and Apache Ant \cite{www:ant} have been developed
that provide dependency management for Java projects, by automatically
downloading Java libraries (in the form of JARs) from online
repositories. A Java project needs only to provide a list of
dependencies, in the form of a well-defined library name and a version
number, and its build tool will handle the rest (such as resolving the
libraries, and downloading the relevant JARs, including any transitive
dependencies they might have).

Aside from the fact that C/C++ is not intended to run on a virtual
machine, there are many other reasons why such automatic dependency
management and distribution of compiled artifacts is not as common as
in Java. To list a few complications:
\begin{itemize}[--]
\item A C/C++ library would also need to distribute its header files,
  to be able to compile against it. There are no header files in Java.
\item Aside from providing several versions of a library for different
  platforms, one would have to provide many versions for different
  binary compatibility standards as well.
\item Due to ABI changes, even different versions of the language
  (e.g., C++98 vs C++11) can break binary compatibility in some cases,
  for code compiled by different compilers or even from different
  versions of the same compiler.
\item By design, Java class files tend to be quite small in size (a
  few kilobytes at most). For instance, size is one of the main
  reasons why Java bytecode is a stack-based representation (i.e., it
  uses a stack instead of variables to contain the operands of each
  instruction).
\item Java has no explicit link phase that combines compilation units
  to form an executable program. All classes are linked dynamically in
  Java (via class loaders), when they are loaded into the JVM. Classes
  are loaded on demand and the runtime system does not need to know
  about specific filesystem paths, at all. You could even compile a
  class against one version of a library, but provide another version
  at runtime, as long as the relevant signatures match.  In C++,
  compilation involves linking as well.
\end{itemize}

All these limitations would make distributing compiled artifacts of
C/C++ marginally better than to distribute the code itself.

However, JARs can also be thought of as partial programs, since they
only contain a subset of the program's classes. For a static analysis
to require a whole program to analyze, would be too restrictive in the
Java world, where JARs are the most easily obtainable artifact. This
raises the question:
\begin{displayquote}
  What are the challenges of statically analyzing partial Java
  programs, as in the form of JAR files, or any non-complete
  (w.r.t. the whole program) collection of class-files?
\end{displayquote}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
