\label{chapter:intro}

Static program analysis is a vast field with broad uses; an umbrella
term for many different methodologies (%
\begin{inparablank}
  \item Hoare logic
    \cite{journals/cacm/Hoare69,floyd1967assigning,lics:2002/Reynolds,csl/OHearnRY01},
  \item model checking
    \cite{icalp/EmersonC80,lop/ClarkeE81,toplas/ClarkeES86,programm/QueilleS82},
  \item symbolic execution
    \cite{journals/cacm/King76,journals/tse/Howden77,conf/kbse/PasareanuR10,Boyer:1975:SFS:390016.808445},
  \item abstract interpretation
    \cite{popl/CousotC77,journals/jlp/CousotC92,journals/logcom/CousotC92},
  \item data-flow analysis
    \cite{popl/Kildall73,books/daglib/0030999,books/mk/Muchnick1997,journals/acta/KamU77,popl/RepsHS95,books/ph/SharirP81},
\end{inparablank}
and so on), that aim to automatically obtain an
understanding of a program's behavior, without running it. Nowadays,
one form or another of static analysis can be found in many different
contexts: compilers, IDEs, editors, linters, or even dedicated static
analysis tools or frameworks. The ends of a static analysis tool are
equally diversified, ranging from bug finding and program
verification to optimization, or even aided program comprehension.

Along with static analysis tools, the programming languages have
evolved as well, becoming more high-level throughout the years,
introducing many layers of abstraction, before eventually translating
the program to the machine's native opcodes. High-level languages are
appealing because they are easier to program in, and maintain. Less
programming effort (e.g., in terms of lines of code) is needed to
express some computation. Virtual machines have even abstracted away
the platform where the code will run. Instead, programs of managed
languages are translated to machine code for some \emph{virtual
  machine}, and hence may run on any platform that provides a backend
that emulates this virtual machine.

Software has evolved too. Complex design patterns, immense libraries,
frameworks implementing inversion of control, over-involved build
tools, and many other complicacies pose significant challenges to
program understanding.

As one would expect, static analyses have struggled to keep up with
the ever-increasing complexity of software and the programming
languages it is written in; the very task of automated program
understanding has become daunting, yet even more valuable.

The most promising and powerful of existing static analysis techniques
rely on the creation of some \emph{abstract memory model} of the
program. What objects will the memory contain, at some state of
execution? What will their structure be like?  A faithful abstract
representation of the actual memory is, however, a demanding task; its
precision often detrimental to the value of whatever the static
analysis is aiming to eventually compute (be it the identification of
complex bug patterns or the opportunities for effective
optimizations).

In this dissertation we argue that there is \emph{implicit structural
  information} in the program, about the memory it will allocate, that
can improve the quality of the abstract memory model constructed by
static analysis. This structural information is not readily available,
but may be recovered via inference, primarily by tracking the use of
types in the program.

We provide a number of techniques that recover such
lost memory structure, in two different settings:
\begin{inparaenum}[(1)]
\item in C/C++ programs, as a typical case of low-level code with
  direct memory access, where the program's memory structure is often
  lost due to specific programming idioms and the inherent low-level
  nature of the language, and
\item in Java programs, where despite the high-level nature of the
  language, structural information may be lost for
  \begin{inparaenum}[(a)]
  \item \emph{partial programs} (i.e., libraries or any programs that
    lack some of their parts), which, in the form of Java Archives
    (JARs), constitute the main distributable code entity of this
    managed language, or
  \item due to Java's \emph{reflection} mechanism, which allows
    runtime inspection of classes, interfaces, fields and methods, and
    can be used to instantiate new objects, invoke methods, get/set
    field values, and so on, without exact static type information
    (e.g., the name of the method to be invoked can be created
    dynamically using plain string operations).
  \end{inparaenum}
\end{inparaenum}


\section{Impact}

In this section, we will briefly explain the main contributions of
this dissertation, from both a scientific and a practical perspective.

\subsection{Scientific Contributions}

A weakly-typed language, such as C or C++, exposes pointers as numeric
values and allows the programmer to perform arbitrary arithmetic on
them. These \emph{pointer arithmetic} capabilities can be used to
bypass the language's type system. Objects may be allocated on memory
without any local information about their intended type, at the
allocation site. In fact, the norm for most heap-allocating routines
(e.g., \code{malloc()}) is to return just an array of bytes. These
allocations, while in this \emph{untyped state}, flow to various other
instructions and may be even stored to type-agnostic raw pointer
variables. Normally, when such an allocation was intended to create an
object of type \(T\), a cast instruction or an implicit conversion
will be used prior to any other instruction that expected an object of
this specific type.

\emph{Pointer analysis} is a static program analysis that determines
the values of pointer variables or expressions. For each pointer, it
computes a set of memory allocations that the pointer may point to. We
refer to this as the \emph{points-to set} of a variable. Since
computing an exact model of memory is undecidable, a static analysis
needs to sacrifice performance for computability. Thus, the memory
allocations of pointer analysis are mere abstractions; a single
allocation may represent many concrete objects at some program
execution. One such popular abstraction represents memory objects by
their allocation sites. Hence, any number of concrete objects
allocated at the same instruction correspond to a single abstract
object.

In the case of C/C++ programs, the first scientific contribution is a
\emph{revised abstract memory model} that differs from the classic
allocation-site abstraction approach, by introducing many more
abstract objects (not just one per allocation site). Such a finer
granularity, in terms of memory abstraction, is a key step for the
analysis to maintain strong type information for its abstract
objects. After all, the same allocation site can be used in C to
create allocations of more than one distinct types. Also, as will be
explained later, C allows a pointer variable to point to some field of
an allocated object. We tackle this by representing fields and array
indices of abstract objects as separate abstract (sub-)objects with
their own points-to sets. Hence, the pointer analysis can
differentiate between pointing to some abstract object, and pointing
to one of its fields (or array indices). This is commonly known as
\emph{field-sensitivity}. Due to C's expos\'{e} of pointers, a
field-sensitive pointer analysis is much harder to implement, than in
a language that doesn't provide direct memory access (e.g., Java). Our
revised memory model aims to extend the domain of abstract objects to
naturally express field sensitivity for C and C++.

The second scientific contribution in the C/C++ setting of this work
is a technique to enhance pointer analysis precision by
``dynamically'' associating and maintaining type information for all
abstract objects in memory.  By the term ``dynamically'', we mean that
any object-type association is performed simultaneously with the
pointer analysis itself (in a manner similar to \emph{on-the-fly}
call-graph construction). The pointer analysis uses the inferred types
of abstract objects to produce new points-to facts or filter spurious
inferences due to type incompatibility. The points-to sets, on the
other hand, drive the creation of new object-type associations that
may again alter the produced points-to sets, and so on---all partaking
in an interdependent recursive cycle of computation.

We use this technique to collect type hints---indications that some
abstract object has type \(T\)---and for each type discovered we
(dynamically again) create a new (typed) variant of the original
abstract object. Thus, the same allocation site may produce multiple
abstract objects for different types, while those types will be
determined on-the-fly (i.e., through the pointer analysis itself). The
plethora of abstract objects generated by this technique is in line
with the fine-grained property of our revised abstract memory model.

As an example of a type hint, which demonstrates how these two
techniques interact, consider a field access \code{((P*) p)->f}. Due
to analysis imprecision, the analysis may be unable to reason about
the type of the abstract object(s) that \code{p} points to (as it
could have been allocated via a generic \code{malloc()} call with no
type indication). Or, it may even have computed that \code{p} points
to objects of incompatible type (that do not contain any \code{f}
field, whatsoever). However, given the present static type
information, the analysis will mark \code{P} as one of the possible
types of the base abstract object \(\alloc{obj}\) (for any
\(\alloc{obj}\) that \code{p} may point to), if the type of the latter
is yet unknown. Other objects with known yet incompatible types will
be filtered out. Thus, the analysis will create a new \emph{typed}
abstract object \(\alloc{obj}_P\) of type \code{P}, which will also
flow to the points-to set of variable \code{p}. This object will now
be eligible as the base address for accessing field \code{f} (type
compatibility is guaranteed by the compiler). Finally, the analysis
will compute that the expression \code{p->f} points to the (typed)
abstract subobject that represents field \code{f} of
\(\alloc{obj}_P\). Hence, at field accesses the analysis will always
be able to recover potentially lost structural information.

In the realm of Java, the challenges are quite different. Java is a
statically and strongly typed language that doesn't expose
pointers. All objects (except that of primitive types) are allocated
in the heap, and accessed via references (allocated on the
stack). References are the disciplined equivalent of a C pointer, and
allow no pointer arithmetic at all. All heap allocations of Java have
a single (dynamic) type, declared at the allocation site. Objects of
composite types can only contain references to other objects and there
is no way to store a reference to an object's field. Hence, pointer
analysis can be expressed via a much simpler memory model, based on the
allocation-site abstraction.

However, Java has another crucial difference from C/C++: it is a
managed language. All Java code is translated to a
platform-independent IR, which is Java bytecode, to be executed by a
Java Virtual Machine (JVM). Using just-in-time (JIT) compilation, the
JVM will translate the bytecode to machine code---more precisely, the
JVM will jit-compile some parts of the bytecode, specifically, the
most frequently called methods or methods with long-running loops
(also known as \emph{hot spots}), and interpret the rest of it
\cite{journals/taco/KotzmannWMRRC08}.

Java also introduces the concept of a Java Archive (JAR), which is a
bundle of class files (compilation units in bytecode format), and
possibly other files as well, using a ZIP file format. Since JARs
contain essentially bytecode, they are platform-independent as
well. Build tools, such as Apache Maven \cite{www:maven}, Gradle
\cite{www:gradle}, and Apache Ant \cite{www:ant} have been developed
that provide dependency management for Java projects, by automatically
downloading Java libraries (in the form of JARs) from online
repositories. A Java project needs only to provide a list of
dependencies, in the form of a well-defined library name and a version
number, and its build tool will handle the rest (such as resolving the
libraries, and downloading the relevant JARs, including any transitive
dependencies they might have).

Aside from the fact that C/C++ is not intended to run on a virtual
machine, there are many other reasons why such automatic dependency
management and distribution of compiled artifacts is not as common as
in Java. To list a few complications:
\begin{itemize}[--]
\item A C/C++ library would also need to distribute its header files,
  so that one would be able to compile against it. There are no header
  files in Java.
\item Aside from providing several versions of a library for different
  platforms, one would have to provide many versions for different
  binary compatibility standards as well (Itanium, MSVC8, MSVC9, etc).
\item Due to ABI changes, even different versions of the language
  (e.g., C++98 vs C++11) can break binary compatibility in some cases,
  for code compiled by different compilers or even from different
  versions of the same compiler.
\item By design, Java class files tend to be quite small in size (a
  few kilobytes at most). For instance, size is one of the main
  reasons why Java bytecode is a stack-based representation (i.e., it
  uses a stack instead of variables to contain the operands of each
  instruction). C++ object files are considerably less compact. An
  alternative IR, specifically designed to reduce code size, could be
  a necessity, to be able to maintain repositories that contain vast
  collections of precompiled libraries.
\item Java has no explicit link phase that combines compilation units
  to form an executable program. All classes are linked dynamically in
  Java (via class loaders), when they are loaded into the JVM. Classes
  are loaded on demand and the runtime system does not need to know
  about specific filesystem paths, at all. You could even compile a
  class against one version of a library, but provide another version
  at runtime, as long as the relevant signatures match.  In C++,
  compilation involves linking as well.

  The only practice that remotely resembles Java's dynamic class
  loading is shared libraries (or dynamic-link libraries (DLLs), in
  Windows). However, those have their own pitfalls. For instance, a
  single unresolved symbol (missing DLL) will forbid the program from
  executing at all. Due to complex dependency chains, even identifying
  the missing DLL is often a difficult task.
  % DLL Hell vs Jar Hell
\end{itemize}

All these limitations would make distributing compiled artifacts of
C/C++ marginally better than to distribute the code itself.

Now that we have established some of the reasons that account for the
prevalence of JARs, we can now switch our focus to static analysis
again. As far as static analysis is considered, JARs can be thought of
as \emph{partial programs}, since they only contain a subset of the
program's classes. For a static analysis to require a whole program to
analyze, would be too restrictive in the Java world, where JARs are
the most easily obtainable artifact, for the aforementioned reasons.

This raises the question:
\begin{displayquote}
  What are the challenges of statically analyzing partial Java
  programs, as in the form of JAR files, or any non-complete
  (w.r.t. the whole program) collection of class files?
\end{displayquote}

One of the main challenges is that any partial program may fail to
satisfy even basic soundness guarantees, as those presumed by the Java
verifier itself. Static analysis tools are rarely robust enough to
analyze such programs without risking a great disruptive effects to
their results---that is, if they succeed at running at all. Handling
\emph{phantom types} (e.g., classes referenced in the partial program,
with missing definitions), for which no structural information exists,
can throw off even basic assumptions or invariants of a static
analyzer.

The most vital aspect of the missing structural information is the
\emph{class hierarchy}, the knowledge of subtyping relationships among
the various types defined in the program. A partial program provides
only a part of the complete class hierarchy; however, many more
subtyping relationships are implied in the code itself. For instance,
calling a known method that expects a parameter of type \code{A}, with
an argument of type \code{AImpl}, implies that \code{AImpl} is a
(transitive) subtype of \code{A}, even in the case that any of the
definitions of these two class types are missing. The (complete)
original class hierarchy is guaranteed to satisfy this
constraint.

Hence, we outline the problem of \emph{class hierarchy
  complementation} of partial Java programs:
\begin{displayquote}
  Given a partial program, how to compute a complete class hierarchy
  that satisfies any implied type constraint, as expressed in the Java
  bytecode specification \cite{Lindholm:1999:JVM:553607}.
\end{displayquote}

To compute such a complete class hierarchy is far from trivial, as one
may initially expect. If not done correctly, we could easily end up
introducing cyclic dependencies between types (e.g., \code{A} is a
subtype of \code{B} and \code{B} is a subtype of \code{A}), which
would violate the language semantics. We can express this problem in
pure graph-theoretic terms. The result is two interesting, if not
fundamental, graph-theoretic problems that could as well arise in
completely different settings due to their genericity:

\begin{description}
\item[Multiple Inheritance.] Given a directed acyclic graph, with a
  subset of ``fixed'' nodes (which correspond to our \emph{known}
  non-phantom classes), and a set of binary path constraints among the
  nodes, how can we extend the graph by adding new edges that do not
  originate from the ``fixed'' nodes, so that
  \begin{inparaenum}[(i)]
  \item the graph remains acyclic, and
  \item all path constraints are satisfied (i.e., for each constraint
    between nodes \(X\) and \(Y\), there exists a path from \(X\) to
    \(Y\) in the final graph).
  \end{inparaenum}
\item [Single Inheritance.] Same as in the previous setting, with one
  more constraint: the output graph should be a directed \emph{tree}
  (instead of a DAG).
\end{description}

As to the solution of the class hierarchy complementation problem, we
provide algorithms to solve it in both the multiple and single
inheritance cases. More specifically,
\begin{inparaenum}[(1)]
\item we present a polynomial-time algorithm that solves any instance
  of the problem in the multiple inheritance setting, as well as a
  proof of correctness.
  %
  For the single inheritance setting,
\item we provide a polynomial-time algorithm for a slightly simplified
  setting (yet practically quite common): that where no phantom
  supertypes for fixed (i.e., \emph{non-phantom}) nodes are allowed.
  %
  For the general (single inheritance) case,
\item we provide an algorithm that may perform an exponential search
  in the worst case, but with many heuristics to improve its
  performance.
  %
  Also, for languages such as Java, with single inheritance but
  multiple subtyping and distinguished class vs.  interface types,
\item we demonstrate how the problem can be decomposed into separate
  single- and multiple-subtyping instances.
\end{inparaenum}

Finally, another ubiquitous feature of Java programs that accounts for
leaked structural information in most types of static analyses is none
other than Java's \emph{reflection}. Reflection allows programmers to
dynamically inspect objects and classes, find out what methods and
fields they declare, and access or modify them in whatever way
possible. Given that a Java program can reflectively obtain a member
of a class object given just run-time strings, for a static analysis
to determine what objects are involved in reflective operations it
would need some form of sophisticated string value analysis at
least. Even that could prove insufficient in cases where the strings
involved come from external sources (e.g., properties files) or are
constructed using such low level operations that cannot be modeled
precisely enough by the value analysis at hand.

A technique that can be used to recover missing types in reflective
operations, without any need for string analysis, is similar to the
one we use in the C/C++ setting to dynamically discover the types of
untyped abstract objects by inspecting their normal flow in the
pointer analysis itself. Specifically, we can treat the casts that
reflectively generated objects flow to as type hints for their
respective class objects, if we lack more precise type
information. The principle is the same: to interleave into the main
points-to analysis, logic that associates types to statically untyped
abstract objects, so that these two analysis components can profit
from their symbiotic relationship (one being both consumer and
producer of the other).


In conclusion, we briefly list the main scientific contributions of
this dissertation in both the C/C++ and Java settings:
\begin{itemize}[--]
\item a revised abstract memory model for field-sensitive points-to
  analysis of C/C++ programs
\item a technique to recover missing structural information and
  enhance C/C++ pointer analysis precision by ``dynamically''
  associating and inferring missing type information for abstract
  objects in memory
\item a technique to recover missing structural information in Java
  programs that use reflection, that works with the same principle but
  targets objects involved in reflective operations
\item the graph-theoretic modeling of the class hierarchy
  complementation problem for partial Java programs
\item algorithms that solve the class hierarchy complementation
  problem for both single and multiple inheritance, as well as Java's
  mixed inheritance (i.e., single inheritance/multiple subtyping)
  setting.
\end{itemize}


\subsection{Practical Contributions}

Aside from the scientific contributions of this work, there are
significant practical aspects as well. A crucial engineering choice of
any static analysis framework is to determine its interaction with the
language's build system(s), if any, and the exact point when the
analyzer can intervene in the software's build cycle in order to
analyze it. This will also have direct repercussions on the nature of
the analysis inputs.

For instance, a static analysis tool may choose to completely ignore
the compilation and build process, and directly analyze \emph{source
  code}---this is an approach most often followed by tools performing
superficial (mostly syntactic) analysis, such as linters. Being able
to analyze software by requiring (only) its source code, can be a
blessing or a curse. From a technical standpoint, source code is often
very difficult to analyze, given that a language is most often
designed to be expressive and may contain a large number of (possibly
redundant) syntactic constructs; plain syntactic sugar. A much more
minimal core, with the same expressiveness, yet easier to analyze, can
often be obtained by some transformation. In fact, the technique of
\emph{lowering} the source language, in a series of steps, to a more
fundamental form with simpler syntax each time is a standard strategy
of compilers, before they finally transform the end result (which,
near the end, should be a very simple IR) to machine code. Thus, a
static analysis tool that directly analyzes source code could benefit
greatly by hooking to the compiler or performing analysis post
compilation.
%
On the other hand, being close to the source code can be valuable for
the tool if it needs to report its findings to the end user. The
identification of a bug can have little to no value, if the programmer
is not able to easily understand how and where it can manifest. Thus,
reporting a bug by identifying it in some low-level IR (that the
programmer knows nothing about) is meaningless, unless the problem can
be traced back to the original source code.
%
Apart from technical matters, another thing to consider is the
availability of the source code. A programmer that uses a static
analysis tool may not be able, or not willing to provide source code
in the first place.

A diametrically opposed alternative is to analyze the final product of
the build process: an \emph{executable binary}. There are more
advantages to such an approach, other than code
(un)availability. First, the WYSINWYX phenomenon (i.e., ``What You See
Is Not What You eXecute'') may account for many missed bugs and
vulnerabilities, when the analysis is performed on source code
\cite{toplas/BalakrishnanR10}. The main reasons for such
discrepancies, are:
\begin{compactitem}[\(\cdot\)]
\item platform-specific compiler choices
\item post-compilation modifications to programs
\item (strictly) undefined behavior that is, however, allowed by the
  compiler
\item dynamically linked libraries (DLLs), which are typically not
  available in source-code form
\item inlined assembly code
\end{compactitem}
Also, analyzing binaries has, in general, wider applicability, since
the same analysis can handle any number of compiled language(s).

Finally, there is a range of options depending on the language being
analyzed, that lie between analyzing source code and binaries. That
is, a static analyzer may opt to target an intermediate representation
(IR), such as Java Bytecode for languages running on the
JVM~\cite{Lindholm:1999:JVM:553607}. The advantages of analyzing Java
bytecode, for instance, are the following:
\begin{compactitem}[\(\cdot\)]
\item Java Bytecode is, syntactically, much simpler than Java and
  hence easier to analyze
\item most libraries are available in bytecode format; thus, the
  analysis does not need to provide stubs that model library behavior
\item the analysis may, in principle, support any language that runs
  on the JVM (since it will be compiled to bytecode)
\end{compactitem}
However, it shares many of the downsides of both the source-code and
binary approaches. The WYSINWYX phenomenon may still arise, and
requiring to have a working build for a project may be too optimistic
in some cases. Hence, all three approaches have their respective
benefits and limitations; none is clearly superior to another.

The first major practical contribution of this work is an
implementation of our techniques for analyzing C/C++ programs in
\cclyzer{}\footnote{\cclyzer{} is publicly available at
  \url{https://github.com/plast-lab/cclyzer}}, a static analysis tool
for LLVM bitcode. LLVM bitcode is a low-level RISC-like intermediate
representation, used by the LLVM compiler infrastructure
\cite{cgo/LattnerA04} that we will thoroughly present in
Chapter~\ref{chapter:structsens}. Hence, instead of analyzing source
or binaries, we chose this IR as our analysis target for reasons
similar to those presented for preferring Java bytecode.

Besides field-sensitivity and our revised abstract memory model to
fully support it, Chapter~\ref{chapter:structsens} introduces a
limited form of array sensitivity, so that the analysis can
differentiate between different array indices in some cases. The
combination of these techniques, all implemented in \cclyzer{}, are
powerful enough to allow analyzing C/C++ programs as though written in
a higher-level language, while maintaining a good level of
precision. Consider the invocation of a virtual method in C++. In LLVM
bitcode, or in any object layout that adheres to the Itanium C++ ABI
\cite{itanium:cxx_abi} for that matter, virtual tables are represented
as constant arrays of function pointers. Also, an object (i.e., class
instance) contains a v-pointer field to its respective v-table. Thus,
a virtual call is translated to a series of instructions:
\begin{compactitem}[\(\cdot\)]
\item a load instruction that dereferences the object's v-pointer to
  get its v-table
\item an instruction that adds a relative offset to the start of the
  v-table, to go to the v-table slot that corresponds to the declared
  method of the call
\item a second load instruction that dereferences this specific
  v-table slot to get the actual (possibly overriden) method that will
  be called
\end{compactitem}

A virtual call in Java bytecode would instead by translated to an
\code{invokevirtual} instruction, without exposing the object layout
internals or the implementation of dynamic dispatch. However, due to
the low-level nature of C/C++, this is not an option for any IR
generic enough to support the full spectrum of the language. Our
analysis is able to precisely resolve the method being called, given
such translations, as long as it can determine the dynamic type of the
receiver object. This is the same level of precision, as one would
expect from a typical points-to analysis targeting Java.

Regarding the Java setting and the class hierarchy complementation
problem, we have implemented JPhantom, a tool that accepts any partial
Java program in the form of a JAR file, and generates a complete
program containing skeletal versions of any referenced missing classes
and interfaces so that the combined result constitutes verifiable Java
bytecode with a complete class hierarchy. This tool does not depend on
a specific analysis being run. Rather, it can be used as a
preprocessing step for any static analysis tool, to allow the analysis
of any partial Java program without having to provide custom solutions
for the class hierarchy complementation problem or deal with missing
references at all.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
