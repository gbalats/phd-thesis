%-------------------------------------------------------------------------------
% Prior abstract
%-------------------------------------------------------------------------------

We present the problem of class hierarchy complementation: given a
partially known hierarchy of classes together with subtyping
constraints (``A has to be a transitive subtype of B'') complete the
hierarchy so that it satisfies all constraints. The problem has immediate
practical application to the analysis of partial programs---e.g., it
arises in the process of providing a sound handling of ``phantom
classes'' in the Soot program analysis framework. We provide
algorithms to solve the hierarchy complementation problem in the
single inheritance and multiple inheritance settings.
% We show that class
% hierarchy complementation is NP-complete for multiple
%inheritance/subtyping while there is a polynomial algorithm for single
% subtyping hierarchies.
We also show that the problem in a language such as Java, with single
inheritance but multiple subtyping and distinguished class
vs. interface types, can be decomposed into separate single- and
multiple-subtyping instances.  We implement our algorithms in a
tool, JPhantom, which complements partial Java bytecode programs so that
the result is guaranteed to satisfy the Java verifier
requirements. JPhantom is highly scalable and runs in mere seconds
even for large input applications and complex constraints (with a maximum
of 14s for a 19MB binary).


%Static whole-program analysis requires the availability of every class
%transitively referenced in a program, even if it is never used. For
%instance, trying to analyze a program P that uses a small part of a
%third-party library L also requires the full code of any library L'
%that different parts of L may reference---the fact that L' is not
%necessary for P cannot be determined before the analysis of P is
%performed. This is a oft-encountered practical challenge, motivating
%mechanisms such as the ``phantom class'' facility in the well-known
%Soot analysis framework for Java.
%
%To address this challenge in full generality we define the problem of
%``program complementation'': given a partial program we seek to
%provide definitions for its missing parts so that the ``complement''
%satisfies all static and dynamic typing requirements induced by the
%code under analysis. This requires satisfying constraints relative to
%methods and fields of the missing classes, as well as subtyping
%constraints and constraints on whether a missing type has to be a
%class or interface. We formulate the problem systematically and offer
%an algorithm that produces solutions for the resulting constraints.
%The result is the articulation of a novel typing problem in the OO
%context, as well as a tool of practical interest. We show that we
%produce practical complements of significant size in a few seconds
%and, in this way, allow the analysis of previously un-analyzable
%partial programs.


%-------------------------------------------------------------------------------
% Introduction
%-------------------------------------------------------------------------------

\section{Introduction}
\label{intro}

Whole-program static analysis is essential for clients that require
high-precision
%in analysis results
and a deeper understanding of program behavior. Modern applications of
program analysis, such as large scale refactoring tools
\cite{journals/software/Dig11}, race and deadlock detectors
\cite{pldi/NaikAW06}, and security vulnerability detectors
\cite{sigsoft/MadsenLF13,uss/GuarnieriL09}, are virtually
inconceivable without whole-program analysis.

For whole-program analysis to become truly practical, however, it
needs to overcome several real-world challenges. One of the somewhat
surprising real-world observations is that whole-program analysis
requires the availability of much more than the ``whole program''.
The analysis needs an overapproximation of what constitutes the
program. Furthermore, this overapproximation is not merely
what the analysis computes to be the ``whole program'' after it
has completed executing. Instead, the overapproximation needs to be
as conservative as required by any intermediate step of the analysis,
which has not yet been able to tell, for instance, that some method
is never called.

Consider the example of trying to analyze a program $P$ that uses a
third-party library $L$. Program $P$ will likely only need small parts
of $L$.  However, other, entirely separate, parts of $L$ may make use
of a second library, $L'$.  It is typically not possible to analyze
$P$ with a whole program analysis framework without also supplying the
code not just for $L$ but also for $L'$, which is an unreasonable
burden. In modern languages and runtime systems, $L'$ is usually not
necessary in order to either compile $P$ or run it under any
input. The problem is exacerbated in the current era of large-scale
library reuse.  In fact, it is often the case that the user is not
even aware of the existence of $L'$ until trying to analyze $P$.

Unsurprisingly, the issue has arisen before, in different guises.  The
FAQ document\footnote{http://www.sable.mcgill.ca/soot/faq.html} of the
well-known Soot framework for Java
analysis \cite{vall99soot,valleerai00optimizing} contains the
question:

\vspace{-1mm}
\begin{quote}
\emph{How do I modify the code in
order to enable soot to continue loading a class even if it doesn't
find some of it[s] references? Can I create a dummy soot class so it can
continue with the load? How?}
\end{quote}

\noindent This frequently asked question does not lead to a solution. The answer
provided is:
\begin{quote}
\emph{You can try -use-phantom-refs but often that
does not work because not all analyses can cope with such
references. The best way to cope with the problem is to find the
missing code and provide it to Soot.}
\end{quote}

The ``phantom refs'' facility of Soot, referenced in the above answer,
attempts to model missing classes (\emph{phantom classes}) by
providing dummy implementations of their methods referenced in the
program under analysis. However, there is no guarantee that the
modeling is in any way sound, i.e., that it satisfies the well-formedness
requirements that the rest of the program imposes on the phantom
class.

Our research consists precisely of addressing the above need in full
generality. \emph{Given a set of Java class and interface definitions,
in bytecode form, we compute a ``program complement'', i.e., skeletal
versions of any referenced missing classes and interfaces so that the
combined result constitutes verifiable Java bytecode.} Our solution to
this practical problem has two parts:

\begin{asparaitem}[$\cdot$]
\item A \emph{program analysis} part, requiring analysis of bytecode and techniques
similar to those employed by the Java verifier and Java
decompilers. This analysis computes constraints involving the missing
types. For instance, if a variable of a certain type $C$ is
direct-assigned to a variable of a type $S$, then $C$ must be a subtype of
$S$.
%, as well as constraints on members (e.g., we may know from
%available bytecode that a missing class has a method with a given
%signature)

\item An \emph{algorithmic} part, solving a novel typing problem, which we call
  the \emph{class hierarchy complementation}, or
  simply \emph{hierarchy complementation}, problem. The problem
  consists of computing a type hierarchy that satisfies a set of
  subtyping constraints \emph{without} changing the direct parents of
  known types.
\end{asparaitem}


%We model all requirements on missing classes or interfaces
%(which we call ``phantom types'', alluding to the Soot terminology) as
%a set of typing constraints.

The algorithmic part of our solution, i.e., solving the hierarchy
complementation problem, constitutes the main novelty of our
approach. The problem appears to be fundamental, and even of a certain
interest in purely graph-theoretic terms. For a representative special
case, consider an object-oriented language with multiple inheritance
(or, equivalently, an interface-only hierarchy in Java or
C\#).\footnote{We avoid the terms ``subclassing'' or ``inheritance''
as synonyms for ``direct subtyping'' to prevent confusion with other
connotations of these terms. In our context, we only care about the
concept of subtyping, i.e., of a (monomorphic) type as a special case
of another. Subtyping can be direct (e.g., when a Java class is
declared to ``extend'' another or ``implement'' an interface) or
indirect, i.e., transitive. We do, however, use the compound terms
``single inheritance'' and ``multiple inheritance'' as they are more
common in the classification of languages than ``single subtyping''
and ``multiple subtyping''.}  A partial hierarchy, augmented with
constraints, can be represented as a graph, as shown in
Figure~\ref{fig:ex0:problem}. The known part of the hierarchy is shown
as double circles and solid edges. Unknown (i.e., missing) classes are
shown as single circles. Dashed edges represent subtyping constraints,
i.e., indirect subtyping relations that have to hold in the resulting
hierarchy. In graph-theoretic terms, a dashed edge means that there is
a path in the solution between the two endpoints. For instance, the
dashed edge from $C$ to $D$ in Figure~\ref{fig:ex0:problem} means that
the unknown part of the class hierarchy has a path from $C$ to
$D$. This path cannot be a direct edge from $C$ to $D$, however: $C$
is a known class, so the set of its supertypes is fixed.

\begin{figure}[t]
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/cgraph2.pdf}
    \subcaption{Constraint Graph}\label{fig:ex0:problem}
  \end{minipage}
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/cgraph2-solution.pdf}
    \subcaption{Solution}\label{fig:ex0:solution}
  \end{minipage}
  \caption[Example of constraints in a multiple inheritance setting]{%
    Example of constraints in a multiple inheritance
    setting. Double-circles signify known classes, single circles
    signify unknown classes. Solid edges (``known edges'') signify
    direct subtyping, dashed edges signify transitive subtyping.}
  \label{fig:ex0}
\end{figure}

In order to solve the above problem instance, we need to compute a
directed acyclic graph (DAG) over the same nodes,\footnote{Inventing
extra nodes does not contribute to a solution in this problem.} so
that it preserves all known nodes and edges, and adds edges \emph{only
to unknown nodes} so that all dashed-edge constraints are
satisfied. That is, the solution will not contain dashed edges
(indirect subtyping relationships), but every dashed edge in the input
will have a matching directed path in the solution
graph. Figure~\ref{fig:ex0:solution} shows one such possible solution.
As can be seen, solving the constraints (or determining that they are
unsatisfiable) is not trivial. In this example, any solution has to
include an edge from $B$ to $E$, for reasons that are not immediately
apparent. Accordingly, if we change the input of
Figure~\ref{fig:ex0:problem} to include an edge from $E$ to $B$, then
the constraints are not satisfiable---any attempted solution
introduces a cycle. The essence of the algorithmic difficulty of the
problem (compared to, say, a simple topological sort) is that we
cannot add extra direct parents to known classes $A$ and $C$---any
subtyping constraints over these types have to be satisfied via
existing parent types. This corresponds directly to our high-level
program requirement: we want to compute definitions for the missing
types only, without changing existing code.

% We show that computing whether such a DAG exists is an
% NP-complete problem, for the case of multiple subtyping.

For a language with single inheritance, the problem is similar, with
one difference: the solution needs to be a tree instead of a DAG. (Of
course, the input in Figure~\ref{fig:ex0:problem} already violates the
tree property since it contains known nodes with multiple known
parents.) We offer an algorithm that solves the problem by
either detecting unsatisfiability or always ordering the nodes in a
tree that respects all constraints.

%\footnote{Although this is true only because we are accepting any
%legal bytecode as input. As we discuss later, if we assume that the
%input is produced by following specific compiler conventions
%then...}

The practical version of the hierarchy complementation problem is more
complex. Mainstream OO languages often distinguish between classes and
interfaces and only allow single direct subtyping among classes and
multiple direct subtyping from a class/interface to an interface---a
combination often called ``single-inheritance, multiple
subtyping''. In this case, the graph representation of the problem is
less intuitive. Consider Figure~\ref{fig:real-example:problem} that
gives a problem instance. (A possible solution for these constraints
is in Figure~\ref{fig:real-example:solution}, but is given purely for
reference, as it is not central to our discussion.) There are now
several node types: classes, interfaces (both known and unknown), as
well as undetermined nodes. There are also more implicit constraints
on them: classes can only have an edge to one other class, interfaces
can only have edges to other interfaces.  The latter constraint, for
instance, forces $D$ to be an interface and $H$ to be a class. Thus, we
see that the full version of the problem requires additional
reasoning. We show that such reasoning can be performed as a
pre-processing step. The problem can be subsequently broken up into
two separate instances of the aforementioned single- and
multiple-inheritance versions of hierarchy complementation.

%merely refines locally the algorithms for solving the
%problem versions for multiple inheritance and

\begin{figure}[t]
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/cgraph3.pdf}
    \subcaption{Constraint Graph}\label{fig:real-example:problem}
  \end{minipage}
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/solution3.pdf}
    \subcaption{Solution}\label{fig:real-example:solution}
  \end{minipage}
  \caption[Example of full-Java constraint graph]{Example of full-Java
    constraint graph. Double circles denote known classes/interfaces,
    whose outgoing edges in the solution are already determined (solid
    input edges). White nodes are classes, black nodes are interfaces,
    grey nodes are unknown types that are initially undetermined
    (i.e., the input does not explicitly identify them as classes or
    interfaces, although constraint reasoning may do so later).}
  \label{fig:real-example}
\end{figure}



% Subsequently, we offer an algorithm for
% solving program complementation constraints. We implement our solution
% for Java, at the bytecode level. Our resulting tool accepts as
% input a set of Java bytecode files and produces a jar file with
% appropriate definitions of all phantom types.

%% The well-formedness guarantee of our solution to the program
%% complementation problem is that we produce classes that could have
%% been produced by the front-end Java compiler (javac) for \emph{some}
%% definition of phantom classes. This offers no guarantee that the
%% resulting original program + complement will not exhibit dynamic type
%% errors, but such a guarantee is not required: the assumption is that
%% phantom classes are indeed \emph{not used} in a meaningful way, and
%% all we want is for them to be consistent with the rest of the
%% program. For instance, a key consistency property of our solution
%% relative to the Soot phantom class treatment is that we respect
%% subtyping: if a subtyping relationship involving a phantom class
%% (or interface) can be inferred from the code then it is satisfied
%% in the complement that we output.

% To see why the problem has interesting depth and complexity, consider
% a simple fragment of Java bytecode and the constraints it
% induces. Our convention throughout the paper is that single-letter
% class names at the lower end of the alphabet (\code{A}, \code{B},
% ...)  correspond to known types, while class names at the high end
% of the alphabet (\code{X}, \code{Y}, \code{Z}) denote phantom types.
% We present bytecode in a slightly condensed form, to make clear what
% method names or type names are referenced in every instruction.
%
% \begin{smallnolinecode}
% \begin{verbatim}
% public void foo(X, Y);
%  0: aload_2     // load on stack 2nd argument (of type Y)
%  1: aload_1     // load on stack 1st argument (of type X)
%  2: invokevirtual X.bar:(LA;)LZ; //method 'Z bar(A)' in X
%  3: checkcast B // cast the result of the call to B
%  ...
% \end{verbatim}
% \end{smallnolinecode}
%
% Although the above fragment is merely four bytecode instructions
% long, it induces several interesting constraints for our phantom
% types \code{X}, \code{Y}, and \code{Z}:
%
% \begin{bullets}
% \item \code{X} has to support a method \code{bar} accepting an
%   argument of type \code{A} and returning a value of type \code{Z}.
%
% \item \code{Y} has to be a subtype of \code{A}, since an actual
%   argument of declared type \code{Y} is passed to \code{bar}, which
%   has a formal parameter of type \code{A}. This constraint also
%   means that if \code{A} is known to be a class (and not an
%   interface) then \code{Y} is also a class.
%
% \item \code{Z} is either a subtype of \code{B} or a supertype of
%   \code{B}.  The reason is that we have a cast from a reference of
%   type \code{Z} to one of type \code{B} and our well-formedness
%   requirements dictate that no dynamic type error (class cast
%   exception) be produced by such a cast. A naive view would suggest
%   that the real constraint induced by the cast is that \code{Z} be a
%   supertype of \code{B} (since the existence of a dynamic cast check
%   implies a narrowing conversion) but the bytecode may elide any
%   number of intermediate casts that existed in the source code. For
%   instance, the source code may have cast the return value of
%   \code{bar} to \code{java.lang.Object} (a cast that is always safe
%   and thus disappears in the bytecode) before casting it to
%   \code{B}. Thus, the real constraint is that \code{Z} (which is
%   only the static type of the reference, and therefore a supertype
%   of the dynamic type of the object) is either a subtype or
%   supertype of \code{B}.
% \end{bullets}
%
% Our approach will create skeletal versions of \code{X}, \code{Y},
% and \code{Z} so that all the above requirements are
% satisfied. Clearly, there can be unsatisfiable instances of the
% above constraints (e.g., cyclic subtyping requirements), and these
% are detected by our algorithm. However, unsatisfiable constraints
% cannot arise in real-world instances of the problem, unless parts of
% the code have changed without recompiling other, dependent parts.

In brief, the contributions of our work are as follows:

\begin{itemize}[--]
\item We introduce a new typing problem, motivated by real-world needs
  for whole program analysis. To our knowledge, the hierarchy
  complementation problem has not been studied before, in any context.
\item We produce algorithms that solve the problem in three different
  settings: single inheritance, multiple inheritance, and mixture of
  the two, as in Java or C\#.
%  For a single inheritance setting, we
%  offer a simple algorithm, possibly of wider applicability to other
%  domains (e.g., partially-ordered sets for which the concept of
%  ``direct predecessor'' needs to be distinguished from merely ``in
%  order''). Based on similar insights, we adapt the algorithm to a
%  multiple inheritance setting.
%  For the practical setting of a single-inheritance,
%  multiple-subtyping language, we offer an algorithm that decomposes
%  the problem into the previous two versions.
%, and also prove
% that the problem is NP-complete.
%  we offer an algorithm
%  that has exponential complexity in the worst case.
\item We implement our algorithms in JPhantom: a practical tool for
  Java program complementation that addresses the soundness
  shortcomings of previous Java ``phantom class'' approaches. We show
  that JPhantom scales well and takes only a few seconds to process
  even large benchmarks with complex constraints---e.g., less than
  6sec for a 3.2MB binary that induces more than 100 constraints.
%, encountering no exponential complexity
%  in practice.
\item We discuss the problem of hierarchy complementation in more
  general settings. The simplicity of our approach is a result of only
  assuming (for the input) and satisfying (for the output) the fairly
  weak Java bytecode requirements. We show that the problem becomes
  harder at the level of the type system for the source language.
\end{itemize}


%-------------------------------------------------------------------------------
% Motivation
%-------------------------------------------------------------------------------

% In the next sections we detail
\section{Motivation and Practical Setting}

We next discuss the practical setting that gives rise to the hierarchy
complementation problem.

Our interest in hierarchy complementation arose from efforts to
complement existing Java bytecode in a way that satisfies the
soundness guarantees of the Java verifier. Consider a small fragment
of known Java bytecode and the constraints it induces over unknown
types. (We present bytecode in a slightly condensed form, to make
clear what method names or type names are referenced in every
instruction.) In this code, classes \code{A} and \code{B} are
available, while types \code{X}, \code{Y}, and \code{Z} are phantom,
i.e., their definition is missing.

\begin{bytecode}
public void foo(X, Y)
0: aload_2     // load on stack 2nd argument (of type Y)
1: aload_1     // load on stack 1st argument (of type X)
2: invokevirtual X.bar:(LA;)LZ; // method 'Z bar(A)' in X
3: invokevirtual B.baz:()V;     // method 'void baz()' in B
 ...
\end{bytecode}

The instructions of this fragment induce several constraints for our phantom
types. For instance:

\begin{itemize}[--]
\item \code{X} has to be a class (and not an interface) since it
  contains a method called via the \code{invokevirtual} bytecode
  instruction.
\item \code{X} has to support a method \code{bar} accepting an
  argument of type \code{A} and returning a value of type \code{Z}.
\item \code{Y} has to be a subtype of \code{A}, since an actual
  argument of declared type \code{Y} is passed to \code{bar}, which
  has a formal parameter of type \code{A}. This constraint also means
  that if \code{A} is known to be a class (and not an interface) then
  \code{Y} is also a class.
\item \code{Z} has to be a subtype of \code{B}, since a method of
  \code{B} is invoked on an object of declared type \code{Z} (returned
  on top of the stack by the earlier invocation).
\end{itemize}

The goal of our JPhantom tool is to satisfy all such constraints and
generate definitions of phantom types \code{X}, \code{Y}, and \code{Z}
that are compatible with the bytecode that is available to the tool
(i.e., exists in known classes). Compatibility with existing bytecode
is defined as satisfying the requirements of the Java verifier, which
concern type well-formedness.

Of these constraints, the hardest to satisfy are those involving
subtyping.  Constraints on members (e.g., \code{X} has to contain a
``\code{Z bar(A)}'') are easy to satisfy by just adding type-correct
dummy members to the generated classes. This means that the problem in
the core of JPhantom is solving the class hierarchy complementation
problem, as presented in the introduction and defined rigorously in
later sections. The binding of the problem to practical circumstances
deserves some discussion, however.

First, note that, in our setting of the problem, we explicitly
disallow modification of known code, e.g., in order to remove
dependencies, or to add a supertype or a member to it. Such
modifications would have a cascading effect and make it hard to argue
about what properties are really preserved. Additionally, we do not
assume any restrictions on the input, other than the well-formedness
condition of being legal Java bytecode (according to the
verifier). Strictly speaking, our well-formedness condition for the
input is defined as follows: \emph{a legal input is bytecode that can
  be complemented (by only adding extra class and interface
  definitions) so that it passes the Java verifier}. Note that this
well-formedness condition does not depend on the program complement
that our approach produces: an input is legal if there is \emph{some}
complement for it, not necessarily the one that JPhantom computes.

%%% We later use this. Better not emphasize it.
%Notably, this well-formedness condition does
%not assume that known classes have known superclasses---e.g.,
%Figure~\ref{fig:real-example:problem} offered an instance where a
%superclass was a phantom type. Of course, an actual usage setting of
%JPhantom can impose this or other restrictions on the input.

% There
%are several good reasons for this decision: First, modification of
%existing code would likely
%-we cannot modify existing code. E.g., adding a supertype induces
%constraints that are not satisfied (e.g., an interface method may not be
%implemented).

A final interesting point concerns the practical impact of the
JPhantom soundness condition. For most program analyses, omitting
parts of the code introduces unsoundness, if we make no other
assumptions about the program or the omitted part. E.g., it is
impossible to always soundly compute points-to information, or
may-happen-in-parallel information when part of the program is
missing. Therefore, guaranteed soundness for all clients is inherently
unachievable for \emph{any} partial program analysis approach. The
practical reality is that there is a large need for facilities for
handling partial programs. For instance, the Soot phantom class
machinery has been one of the most common sources of discussion and
questions on the Soot support lists, and it has been a central part of
several Soot revisions.\footnote{Even the most recent Soot release,
2.5.0, lists improved support for phantom classes and excluding
methods from an analysis as one of the major changes in the release
notes.} The only ``correctness condition'' that Soot phantom class
support is trying to achieve, however, is the low-level ``the analyzer
should not crash''.

Given the practical interest for the solution of a worst-case
unsolvable problem, we believe that our soundness guarantee makes a
valuable contribution: it is much better to analyze a partial program
in a way such that the Java verifier requirements (for type-level
well-formedness) are satisfied than to ignore any correctness
considerations, as past approaches do.

% unsatisfiable constraints cannot arise in
%real-world instances of the problem, unless parts of the code have
%changed without recompiling other, dependent parts.


%-------------------------------------------------------------------------------
% Multiple Inheritance
%-------------------------------------------------------------------------------

\section{Hierarchy Complementation for Multiple Inheritance}

\label{multiple}

We begin with a modeling of the hierarchy complementation problem in
the setting of multiple inheritance. This means that every class in our
output can have multiple parents.

%% Formalization

We can model our problem as a graph problem. Our input is a directed
graph $G = (V,E)$, with two disjoint sets of nodes $V = V_{known} \disjunion
V_{phantom}$ and two disjoint sets of edges $E = E_{direct} \disjunion
E_{path}$, where $E_{direct} \subseteq V_{known} \times V$ (i.e.,
direct edges have to originate from known nodes---the converse is not
true, as known nodes can be inferred to subtype unknown ones due to
assignment instructions in the bytecode). The set of nodes $V$ is a
set of types, while the set of edges $E$ corresponds to our subtyping
constraints. That is, an edge $(v_s,v_t)$ encodes the constraint $v_s
<: v_t$. The $E_{direct}$ subset encodes the direct-subtype
constraints. The output of our algorithm should be a \emph{DAG} (with
edges from children to their parents), $G_D = (V,E')$, such that:
\begin{enumerate}
\item $\forall v_s \in V_{known}: (v_s,v_t) \in E' \Leftrightarrow (v_s,
 v_t) \in E_{direct}$ (i.e., all direct edges from known nodes are preserved
and no new ones are added to such nodes)
\item $(v_s,v_t) \in E_{path} \Rightarrow$ there is a path from $v_s$ to
$v_t$ in $G_D$
\end{enumerate}

%Again, we could decouple path-edges from direct-edges by introducing
%the input and output mappings, $f_i:V_{known} \rightarrow 2^V$,
%$f_o:V \rightarrow 2^V$. In both cases, these two mappings map a type
%(key) to its \emph{set} of outgoing edges (value). This way, $G_C =
%(V,E)$ will only contain path-edges such that $\forall (x_1,x_n) \in E
%:$ there must exist a path $(x_1,x_2,\ldots,x_n) \text{ in }
%G_D$ (second constraint). Additionally, as in the single inheritance case, we must ensure
%that $\forall c \in V_{known}: f_o[c] = f_i[c]$ (first constraint).

Note that our only limiting constraint here is that we cannot have
cycles in the resulting hierarchy. Moreover, since each type may have
multiple supertypes in this setting, a directed acyclic graph is
fitting as our intended output.

%% Phantom-only case

In contrast to the general case, the problem is trivial if we have a
phantom-only input, i.e., if we ignore $V_{known}$ and
$E_{direct}$. It suffices to employ a cycle-detection algorithm,
and---if no cycles are present---return the input constraint graph as
our solution: all path edges can become direct subtyping edges. If our
input graph contains a cycle, then our problem is unsolvable.  If
not, our solution would probably contain some redundant edges (i.e.,
edges connecting nodes that are already connected by another path)
that we could prune to minimize our output. In either case, our
solution would be valid w.r.t. our constraints.

%Optionally, we could prune some edges from our
%solution without breaking any constraints, to minimize the output of
%our algorithm for practical
%purposes. Algorithm~\ref{alg:multiple:simple} performs such an
%optimization, by first computing a transitive closure of the
%constraints and then removing redundant edges, whose target can be
%reached from another path.

%% Projection Sets

The problem becomes much more interesting when we take $V_{known}$
into account. The source of the difficulty is the combination of cycle
detection with nodes whose outgoing edge set cannot be extended.
Consider first the pattern of Figure~\ref{fig:choice}.

\begin{figure}[htp]
  \centering \includegraphics[scale = 0.6]{figures/complementation/minimum.pdf}
  \caption[Multiple Inheritance Constraint]{In any solution of these
    constraints, either $B$ or $C$ or $D$ have to be ordered below
    $E$, since no new outgoing edges can be added to $A$ and the path
    constraint to $E$ needs to be satisfied.}
\label{fig:choice}
\end{figure}

This pattern is a basic instance of interesting reasoning in the case of
multiple inheritance. We have $A \in V_{known}$ such that $(A,B),
(A,C), (A,D) \in E_{direct}$ and $(A,E) \in E_{path}$. We cannot,
however, satisfy the path ordering constraint by adding edges to the
known node $A$. Therefore the output must have one of $B,C,D$ ordered
below $E$. We refer to the set of $\{B,C,D\}$ as the \emph{projection
  set} of node $A$, which is a more generally useful concept.

\begin{defn}{\emph{Projection Set.}}
%  Let $G = (V,E) : V = V_{known} \disjunion V_{phantom}$ and $E
%  = E_{path} \disjunion E_{direct}$ be a constraint graph.
  A node $t \in V_{phantom}$ belongs to the \emph{projection set} of a
  node $s \in V_{known}$ iff $t$ is reachable from $s$ through a path
  of \emph{direct} edges.
  %% Redundant: (a path of direct edges will always contain known nodes
  %% internally)
  %% that internally contains only known nodes.
  %%
  %% We need the definition below since it is used in the algorithm.
 $$ \textit{proj}(s) \equiv \{t \in V_{phantom}: (s,t) \in {E_{direct}}^+\}$$
 with the $+$ symbol denoting transitive closure.
  %% This is wrong!
 %% $$ proj(s,t) \equiv \{(s,t): t \in V_{phantom} \land (s,t) \in
 %% {E_{direct}}\}^+$$ with the $+$ symbol denoting transitive
 %% closure.
\end{defn}

That is, for each known node we can follow its outgoing direct-edges
recursively, ending each path when we reach a phantom node. For
instance, in Figure~\ref{fig:proj}, the phantom projection set for
node $A$ is $\{C,E,H\}$.

% Node $I$ is not included since no path exists
% from $A$ to $I$ through direct-edges only.

Referring again to Figure~\ref{fig:proj}, we can see that if $H$ is
chosen from the projection set of $A$ in order to satisfy the
path-edge $(A,B)$, and therefore edge $(H,B)$ is added, then this
would immediately create a cycle because of the existing $(B,H)$
edge. Our algorithm should prevent such a cycle by making the correct
choice from the relevant projection set.
% Furthermore, we can also see why it is redundant to add $I$ in the
% projection set of $A$. Had node $C$ created a cycle if chosen to
% satisfy the path to node $B$ (or any other node), it would mean that
% a path from $B$ to $C$ exists, and so it would be impossible to
% satisfy the same constraint through $I$ (since there is also a path
% from $B$ to $I$, through node $C$).

\begin{figure}[tp]
  \centering
  %%\subfloat[Constraint Graph]{
    \includegraphics[scale=0.6]{figures/complementation/projections.pdf}
    %%\label{fig:proj:graph}
  %%}
  %% \subfloat[Solution]{
  %%   \includegraphics[scale=0.4]{images/stree1-ext.pdf}
  %%   \label{fig:cgraph1:ext}
  %% }
  \caption[Phantom Projection Set]{%
    The phantom projection set of $A$ is $\{C,E,H\}$. In order to
    satisfy path-edge $(A,B)$ we can either add a path-edge $(C,B)$,
    $(E,B)$, or $(H,B)$. The last one creates a cycle.
  }
\label{fig:proj}
\end{figure}

%% Multiple Inheritance Examples

Combining this projection set choice with cycle detection leads to
interesting search outcomes. Figure~\ref{fig:unsat} shows an example
of unsatisfiable input. The path edge $(B,D)$ makes either $E$ or $F$
be subtypes of $D$, and similarly the path edge $(A,C)$ makes either
$E$ or $F$ be subtypes of $C$. Nevertheless, any choice leads to
cycles. In contrast, Figure~\ref{fig:multipleHard} shows an input for
which a solution is possible, and which we use to illustrate our
algorithm.

\begin{figure}[htp]
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/multipleUnsat.pdf}
    \subcaption{Unsatisfiable input}\label{fig:unsat}
  \end{minipage}
  \begin{minipage}[b]{.5\linewidth}
    \centering
    \includegraphics[scale=0.6]{figures/complementation/multipleEx.pdf}
    \subcaption{Satisfiable input}\label{fig:multipleHard}
  \end{minipage}
  \caption{Multiple Inheritance Examples}
\end{figure}

%%% WRONG!!!
%%%%
% In the same vein, the example of Figure~\ref{fig:multipleHard} is
% satisfiable, but requires some reasoning to find a solution. The path
% edge $(B,H)$ cannot be satisfied by ordering $E$ below $H$ without
% introducing a cycle, but can be satisfied by ordering $F$ below $H$.
%%%%%

%% Algorithm explanation

Algorithm~\ref{alg:multiple:strat} solves in polynomial time (an easy bound is
$O(|V| \cdot |E|)$) any
instance of the hierarchy complementation problem in the multiple
inheritance setting. The main part of the algorithm is
function \textsc{stratify()}, which computes a stratification with the
property that any constraint edge is facing upwards (i.e., from a
lower to a higher stratum). Moreover, this stratification ensures
that, for any path-edge $(s,t)$ originating from a known node, there
will exist a phantom node $p$ in the projection set of $s$ that is
placed lower than $t$. Given this stratification, it is easy to
compute the final solution (as in function~\textsc{solve()}). To
satisfy any such path-edge $(s,t)$, we add a direct-edge from $p$ to
$t$. This respects our invariant of all edges facing upwards, thus
ensuring that no cycles will be present in our solution.

Function~\textsc{stratify()} starts from a single stratum, and then
computes on each iteration a new stratification, $S_{i+1}$, by
building on the stratification of the previous step, $S_{i}$, and
advancing some nodes to a higher stratum in order to satisfy
constraints. This process is repeated until we converge to the final
stratification, which will respect all of our constraints
(line~\ref{lst:line:fixpoint}). If no new node converges at some step
(i.e., all nodes that reached a certain stratum advance to the next),
then we can be certain that we are dealing with unsatisfiable input,
and terminate, thus avoiding infinite recursion
(line~\ref{lst:line:unsat}). The nodes to be advanced at each step are
determined at line~\ref{lst:line:formula}, which captures the essence of
the algorithm. The new stratum of a node
$t$ will be either (i) its current stratum, (ii) the stratum right
above the source of an edge $(s,t)$, or (iii) the one right above the
\emph{lowest} projection node of the source of a path-edge $(s,t)$
originating from a known node---whichever is higher. These conditions
raise the stratum of a node to the minimum required to satisfy the
natural constraints of the problem, per our above discussion:
edges in the solution should be from lower to higher strata.

%% Stratification Example

\begin{figure*}[t]
  \vspace{-5mm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-0-crop.pdf}
    \subcaption{Step 1}\label{fig:multiple:steps:0}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-1-crop.pdf}
    \subcaption{Step 2}\label{fig:multiple:steps:1}
  \end{minipage}
  \\
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-2-crop.pdf}
    \subcaption{Step 3}\label{fig:multiple:steps:2}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-3-crop.pdf}
    \subcaption{Step 4}\label{fig:multiple:steps:3}
  \end{minipage}
  \\
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-4-crop.pdf}
    \subcaption{Step 5}\label{fig:multiple:steps:4}
  \end{minipage}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.4]{figures/complementation/steps-5-crop.pdf}
    \subcaption{Step 6}\label{fig:multiple:steps:5}
  \end{minipage}
  \caption[Stratification Example]{An example of the stratification
    produced by the multiple-inheritance solver for
    Example~\ref{fig:multipleHard}.}
  \label{fig:multiple:steps}
\end{figure*}

\input{algorithms/multiple/stratifying}


Figure~\ref{fig:multiple:steps} presents an illustration of the
algorithm's application to the example of
Figure~\ref{fig:multipleHard}. The sets $\{C,D\}$ and $\{E,F\}$ are
the projection sets of nodes $A$ and $B$ respectively. At the first
step, all nodes will be placed in the lowest stratum. Note that, at
this point, all nodes could be placed in topological order:
Figure~\ref{fig:multiple:steps:0} is perfectly valid as the output
of a topological sort. However, this is not a
solution by our standards, since node $A$ cannot satisfy the edge to
$G$ because both of its projection nodes, $D$ and $C$, are placed
after $G$. Adding an edge from either one would be subject to creating
cycles. At the next step, our algorithm advances every node except $A$
and $B$, since all are edge targets. At step 3, things become more
interesting. Nodes $D,C$ have to be advanced by the same criterion,
since node $H$ contains edges to both, and they all reside in the same
stratum at step 2. However, nodes $H$ and $G$ have to be advanced for
a different reason, since they are targets of path-edges originating
from known nodes, namely $A$ and $B$, whose projections ($\{D,C\}$ and
$\{E,F\}$ respectively) were on the second stratum during the previous
step. At step 4, this condition ceases to exist for node $H$, since
nodes $E,F$ have ``stabilized'' at a lower stratum. This in turn
causes node $D$ to stabilize at step 5. At step 6, $G$ can also stay
put, since it is in a higher stratum than the lowest projection of
$A$, namely $D$. No nodes are advanced at step 7 (which is omitted in
Figure~\ref{fig:multiple:steps}), thus signifying that our
stratification has successfully converged to its final form. It is
therefore simple to compute a solution, by adding edges $(H,D),
(H,C), (G,C)$, $(D,G)$ and either $(F,H)$ or $(E,H)$
to the direct-edges $(A,C), (A,D), (B,E), (B,F)$. This set of edges
will constitute our final solution.

It is also easy to see that our algorithm would soundly detect that
the example of Figure~\ref{fig:unsat} is unsatisfiable. At the first
step, only known nodes $A,B$ would remain in the lowest stratum, but
on the next iteration all remaining nodes would advance again, thus
triggering the condition of failure (line~\ref{lst:line:unsat}), since
an iteration passed with no progress made.

% This projection set will serve as the domain for each path-edge that
% has not been satisfied yet. That is, in order to satisfy a path-edge
% $(s,t)$ we try to add a path-edge from a node in the projection set of
% $s$ to $t$. This may eventually lead to a cycle when all the edges in
% our constraint set have been transformed, in which case another
% candidate for the path-edge that was removed last from our constraint
% set is chosen. If none of the candidates for this path-edge succeeds,
% then our algorithm backtracks to a previous constraint and picks
% another node to satisfy it. If this process exhausts all combinations
% the search for a solution fails.
% %may end up trying all
% %possible combinations, which results in exponential time complexity in
% %the worst-case. In practice however, we will have to examine only a
% %small number of constraints, so this process will hardly be a
% %bottleneck.

A detailed proof of the correctness of our algorithm can be found in
Appendix~\ref{correctness}.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
