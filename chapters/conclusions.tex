\label{chapter:conclusions}
\epigraph{That rug really tied the room together.}{\textit{The Dude}}

In the final chapter, we shall assess our initial thesis and conclude,
while also considering interesting directions for future work.

Our thesis states that there exists \emph{implicit structural
  information} in the program, about the memory it will allocate,
which we can recover via inference and use it to improve the quality
of a static analysis (by coming up with a better abstract memory
model). In Chapter~\ref{chapter:structsens}, we examined the problem
of pointer analysis in C/C++, as a typical example of a low-level
language with direct memory access. We identified particular causes of
analysis imprecision, in untyped allocations (via generic
\code{malloc}-like allocation routines) and in the language's
capability of allowing pointers to point \emph{inside} an object
(i.e., to point to one of its fields or array elements), which further
complicates the modeling of field sensitivity.

We presented a structure-sensitive pointer analysis that may achieve
better precision by changing its abstraction strategy: when the same
allocation site is used to create objects of many different types, the
analysis comes up with many different strongly-typed abstract objects
to represent the same instruction. Also, each different field and
array element of each such strongly-typed object (of a complex type)
will be represented by a unique abstract subobject itself, having its
own separate points-to set, while also maintaining strong type
information. The final outcome is a much more fine-grained allocation
abstraction than that of the typical C/C++ points-to analysis, that is
guided primarily by the flow of types. The key to this technique is
that tracking the flow of types that will determine what objects will be
created is not determined before the analysis has run, but
simultaneously (``on-the-fly''), thus yielding better results due to
the recursive nature of the computation.

This is a prime example of how complex inference (in this case, the
relations computed by the pointer analysis itself) that tracks the use
of types in the program can be used to recover structural information,
namely, the various structures that may be used to access or modify
any untyped heap allocation in memory. The recovered structural
information, in turn, improves the quality of the analysis, in terms
of precision, as shown in the evaluation section of
Chapter~\ref{chapter:structsens}.

In Chapter~\ref{chapter:reflection}, we shift our focus to
higher-level languages like Java, with no pointer arithmetic or direct
memory access capabilities. However, even in such a setting, our thesis
applies: we can improve the quality of a pointer analysis (now, in
terms of empirical soundness) by recovering structural information for
objects involved in reflective operations. By using reflection, Java
programs can encompass dynamic behavior, and as such, are difficult to
statically analyze.

The challenges with reflection idioms are quite similar to those of
analyzing C/C++ that we have already encountered:
\begin{itemize}[\(\cdot\)]
\item the same (reflective) allocation site can be used to allocate
  objects of many different types
\item no local type information exists for reflective objects;
  instead, the types are determined by runtime-strings that are passed
  on as arguments to the reflective operations involved
\item apart from normal object allocations, there are also reflective
  calls that return special objects representing program classes,
  fields, and methods; a reflective call on a method object is much
  like a C/C++ call via a function pointer.
\end{itemize}

To tackle the limitations of existing pointer analyses, induced by
reflection, we presented a reflection analysis (which also runs
simultaneously to the core points-to analysis) that employs similar
techniques to those of Chapter~\ref{chapter:structsens}: by inspecting
the use of reflective objects (i.e., what types they are cast to, what
fields they access, and so on), the analysis is able to recover
essential parts of the objects' structure.
%
By comparing the statically computed call-graphs against dynamically
computed ones from actual executions, we find that these techniques
improve empirical soundness (i.e., the static analysis is now able to
cover more of the actual dynamic behavior of the program than
before).

As already noted, reflection is but one of the possible causes of lost
memory structure, when statically analyzing Java programs. In
Chapter~\ref{chapter:hiercomp}, we examine how to recover lost
structural information for partial Java programs.

Regarding such need, dynamic class loading allows programs to depend
on an abundance of external libraries, without actually requiring all
of them to execute. Transitive dependencies (i.e., the dependencies of
the libraries directly used by the program) make things even worse,
from the perspective of a static analyzer, since such dependencies are
much more numerous and less likely to be actually used. Hence, whole
programs might be both prohibitively difficult to obtain and
superfluous; if some parts of the program were missing, it would make
no difference in any actual execution, had these parts been truly
redundant. In such cases, missing these parts should also make no
difference in analyzing the program too, which explains why there is a
strong incentive for static analysis tools in being able to analyze
partial programs. Despite such incentive, static analysis tools are
rarely well-equipped or tolerant enough to be able to cope with
partial programs. When they do so, they risk a great deterioration to
their results.

To this end, we have presented a generic complementation approach, in
Chapter~\ref{chapter:hiercomp}, that transforms a partial program to a
whole one, by adding stubs for missing code. To create this
complement, we analyze the program and track the use of any phantom types
therein (i.e., types being used but not defined in the code that is
available), to recover their lost structure. Inferring missing members
of phantom types is straightforward. The primary challenge, however,
lies in computing the missing parts of the type hierarchy, in a way
that satisfies all the implied subtyping constraints.

We have defined the type hierarchy complementation problem as follows:
given a partially known hierarchy of classes together with subtyping
constraints (``A has to be a transitive subtype of B'') complete the
hierarchy so that it satisfies all constraints. We presented
algorithms to solve this problem in various inheritance settings
and  implemented these algorithms in JPhantom, a program
complementation tool for Java bytecode.

We evaluated JPhantom on synthetic and real-world benchmarks of
significant size and complexity. JPhantom is highly scalable and runs
in mere seconds even for large input applications and complex
constraints (with a maximum of 14s for a 19MB binary).
%
Lastly, we used the synthetic benchmark (based on the \emph{antlr}
parser generator) to evaluate the difference between analyzing a
partial program as is, and analyzing it after being complemented by
JPhantom. The comparison demonstrated that the latter is much closer
to analyzing the original whole program: analyzing the partial program
without complementation fails to find a great number of reachable
methods (which will not be analyzed at all). Therefore, recovering the
structural information of phantom types, via our program
complementation technique, has improved the static analysis on the
partial program, thus enforcing our thesis.

We believe that the hierarchy complementation problem is fundamental
and is likely to arise in different settings in the future, hopefully
aided by our modeling of the problem and some of its solution avenues.

% To summarize, 

% TODO put in conclusions
% We presented a structure-sensitive points-to analysis for C and
% C++. The analysis attempts to always distinguish abstract objects and
% assign them a unique type (even when none is known at the point of
% object creation) as well as to discriminate between subobjects of a
% single object (array or structure instance). We describe the analysis
% in precise terms and show that its approach succeeds in maintaining
% precision when analyzing realistic programs. In our experience, the
% techniques we described are essential for analyzing C/C++ programs at
% the same level of precision as programs in higher-level languages.

% TODO put in conclusions

% \section{Conclusions}

% %Reflection is of key importance, yet very hard to handle in static
% %analysis.  We presented powerful techniques that elegantly extend
% %declarative reasoning over reflection calls and inter-procedural
% %object flow.

% Highly dynamic features, such as reflection and dynamic loading, are
% the bane of static analysis. These features are not only hard to
% analyze well, but also ubiquitous in practice, thus limiting the
% practical impact of static analysis. We presented techniques for
% static reflection handling in Java program analysis. Our techniques
% build on top of state-of-the-art handling of reflection in Java, by
% elegantly extending declarative reasoning over reflection calls and
% inter-procedural object flow. Our main emphasis has been in achieving
% higher empirical soundness, i.e., in having the static analysis truly
% model observed dynamic behaviors. Although full soundness is
% infeasible for a realistic analysis, it is possible to produce
% general techniques that enhance the ability to analyze reflection calls.

% Although our techniques improve on the problem of handling reflection,
% further work is necessary to achieve
% good scalability and empirical
% soundness for complex programs.
% and sophisticated analysis algorithms.
% Furthermore, our work has not addressed another major and
% commonly used dynamic feature: dynamic loading. Continued work will
% hopefully make such language features a lot more feasible to analyze
% statically.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
