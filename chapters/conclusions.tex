\label{chapter:conclusions}
\epigraph{That rug really tied the room together.}{\textit{The Dude}}

In the final chapter, we shall assess our initial thesis and conclude,
while also considering interesting directions for future work.

Our thesis states that there exists \emph{implicit structural
  information} in the program, about the memory it will allocate,
which we can recover via inference and use it to improve the quality
of a static analysis (by coming up with a better abstract memory
model). In Chapter~\ref{chapter:structsens}, we examined the problem
of pointer analysis in C/C++, as a typical example of a low-level
language with direct memory access. We identified particular causes of
analysis imprecision, in untyped allocations (via generic
\code{malloc}-like allocation routines) and in the language's
capability of allowing pointers to point \emph{inside} an object
(i.e., to point to one of its fields or array elements), which further
complicates the modeling of field sensitivity.

We presented a structure-sensitive pointer analysis that may achieve
better precision by changing its abstraction strategy: when the same
allocation site is used to create objects of many different types, the
analysis comes up with many different strongly-typed abstract objects
to represent the same instruction. Also, each different field and
array element of each such strongly-typed object (of a complex type)
will be represented by a unique abstract subobject itself, having its
own separate points-to set, while also maintaining strong type
information. The final outcome is a much finer-grained allocation
abstraction (than that of the typical C/C++ points-to analysis) that is
guided primarily by the flow of types. The key to this technique is
that tracking the flow of types that will determine what objects will
be created is not determined before the analysis has run, but
simultaneously (``on-the-fly''), thus yielding better results due to
the recursive nature of the computation. We describe the analysis in
precise terms and show that its approach succeeds in maintaining
precision when analyzing realistic programs.

This is a prime example of how complex inference (in this case, the
relations computed by the pointer analysis itself) that tracks the use
of types in the program can be used to recover structural information,
namely, the various structures that may be used to access or modify
any untyped heap allocation in memory. The recovered structural
information, in turn, improves the quality of the analysis, in terms
of precision, as shown in the evaluation section of
Chapter~\ref{chapter:structsens}. In our experience, the techniques we
described are essential for analyzing C/C++ programs at the same level
of precision as programs in higher-level languages.

In Chapter~\ref{chapter:reflection}, we shift our focus to
higher-level languages like Java, with no pointer arithmetic or direct
memory access capabilities. However, even in such a setting, our thesis
applies: we can improve the quality of a pointer analysis (now, in
terms of empirical soundness) by recovering structural information for
objects involved in reflective operations. By using reflection, Java
programs can encompass dynamic behavior, and as such, are difficult to
statically analyze.

The challenges with reflection idioms are quite similar to those of
analyzing C/C++ that we have already encountered:
\begin{itemize}[\(\cdot\)]
\item the same (reflective) allocation site can be used to allocate
  objects of many different types
\item no local type information exists for reflective objects;
  instead, the types are determined by run-time strings that are passed
  on as arguments to the reflective operations involved
\item apart from normal object allocations, there are also reflective
  calls that return special objects representing program classes,
  fields, and methods; a reflective call on a method object is much
  like a C/C++ call via a function pointer.
\end{itemize}

To tackle the limitations of existing pointer analyses, induced by
reflection, we presented a static reflection analysis (which also runs
simultaneously with the core points-to analysis) that employs similar
techniques to those of Chapter~\ref{chapter:structsens}: by inspecting
the use of reflective objects (i.e., what types they are cast to, what
fields they access, and so on), the analysis is able to recover
essential parts of the objects' structure. Our techniques build on top
of state-of-the-art handling of reflection in Java, by elegantly
extending declarative reasoning over reflection calls and
inter-procedural object flow. Our main emphasis has been in achieving
higher empirical soundness, i.e., in having the static analysis truly
model observed dynamic behaviors.

By comparing the statically computed call-graphs against dynamically
computed ones from actual executions, we find that these techniques,
indeed, improve empirical soundness (i.e., the static analysis is now
able to cover more of the actual dynamic behavior of the program than
before). Although full soundness is infeasible for a realistic
analysis, it is possible to produce general techniques that enhance
the ability to analyze reflection calls.

As already noted, reflection is but one of the possible causes of lost
memory structure, when statically analyzing Java programs. In
Chapter~\ref{chapter:hiercomp}, we examine how to recover lost
structural information for partial Java programs.

% Static whole-program analysis requires the availability of every class
% transitively referenced in a program, even if it is never used. For
% instance, trying to analyze a program P that uses a small part of a
% third-party library L also requires the full code of any library L'
% that different parts of L may reference---the fact that L' is not
% necessary for P cannot be determined before the analysis of P is
% performed. This is a oft-encountered practical challenge, motivating
% mechanisms such as the ``phantom class'' facility in the well-known
% Soot analysis framework for Java.

Regarding such need, dynamic class loading allows programs to depend
on an abundance of external libraries, without actually requiring all
of them to execute. Transitive dependencies (i.e., the dependencies of
the libraries directly used by the program) make things even worse,
from the perspective of a static analyzer, since such dependencies are
much more numerous and less likely to be actually used. Hence, whole
programs might be both prohibitively difficult to obtain and
unnecessary at the same time; if some parts of the program were
missing, they would make no difference in any actual execution, had
these parts been truly redundant. In such cases, missing these parts
should also make no difference in analyzing the program, which
explains why there is a strong incentive for static analysis tools to
be able to analyze partial programs. Despite such incentive, static
analysis tools are rarely well-equipped or error-tolerant enough to be
able to cope with partial programs. When they do so, they risk a great
deterioration of their results.

% To address this challenge in full generality we define the problem
% of ``program complementation''
To this end, we have presented a generic complementation approach, in
Chapter~\ref{chapter:hiercomp}, that transforms a partial program to a
whole one, while also seeking to provide definitions for its missing
parts so that the ``complement'' satisfies all static and dynamic
typing requirements induced by the code under analysis. This requires
satisfying constraints relative to methods and fields of the missing
classes, as well as subtyping constraints and constraints on whether a
missing type has to be a class or interface.
%
To identify such constraints, we analyze the program and track uses
of any phantom types therein (i.e., types being used but not defined
in the code that is available), so that we can recover their
lost structure. Inferring missing members of phantom types is
straightforward. The primary challenge, however, lies in computing the
missing parts of the type hierarchy, in a way that satisfies all the
implied subtyping constraints.

We have defined the type hierarchy complementation problem as follows:
given a partially known hierarchy of classes together with subtyping
constraints (``A has to be a transitive subtype of B'') complete the
hierarchy so that it satisfies all constraints. We formulate the
problem systematically and offer algorithms to solve it in various
inheritance settings. The result is the articulation of a novel typing
problem in the OO context.

The entire complementation approach, including the algorithms to solve
the type hierarchy complementation problem are implemented in
JPhantom, a program complementation tool of practical interest for
Java bytecode.
%
We evaluated JPhantom on both synthetic and real-world benchmarks to
show that we produce practical complements of significant size in a
few seconds and, in this way, allow the analysis of previously
un-analyzable partial programs.

Lastly, we used the synthetic benchmark (based on the \emph{antlr}
parser generator) to evaluate the difference between analyzing a
partial program as is, and analyzing it after being complemented by
JPhantom. The comparison demonstrated that the latter is much closer
to analyzing the original whole program: analyzing the partial program
without complementation fails to find a great number of reachable
methods (which will not be analyzed at all). Therefore, recovering the
structural information of phantom types, via our program
complementation technique, has improved the static analysis on the
partial program, thus reinforcing our thesis.

We believe that the hierarchy complementation problem is fundamental
and is likely to arise in different settings in the future, hopefully
aided by our modeling of the problem and some of its solution avenues.

To summarize, we advocate that there are many opportunities of
recovering implicit structural information about memory that can
improve the static analysis of programs, but require complex inference
that takes advantage of indirect uses of types. We have examined three
different scenarios to test and evaluate our thesis, regarding
\begin{inparablank}
\item generic C/C++ programs, and
\item Java programs that either use reflection or
\item are missing parts of their code.
\end{inparablank}
In all cases, we where able to improve static analysis, by recovering
memory structure that was not previously evident.

% Highly dynamic features, such as reflection and dynamic loading, are
% the bane of static analysis. These features are not only hard to
% analyze well, but also ubiquitous in practice, thus limiting the
% practical impact of static analysis.

% +-----------------------------------------------------------------------------
% |  Future Work
% +-----------------------------------------------------------------------------

\section{Future Work}

Finally, we will discuss some interesting future directions to tackle
existing limitations of our approaches.

\subsection{Flow-Sensitivity and Strong Updates}

To begin with, our structure-sensitive points-to analysis for C/C++
lacks flow sensitivity, which is a useful feature for C-like
languages. Flow sensitivity takes the order of program instructions
into account, to increase analysis precision. The results it computes
are now specialized to a specific program point, at which they
hold. Thus, different program points will be associated with different
points-to results, by the end of an analysis run.

We have already discussed, in Chapter~\ref{chapter:structsens}, the
SSA transformation built in the LLVM bitcode format. SSA can be viewed
as a limited form of flow sensitivity, since it ensures that a
variable is only assigned once; a variable with multiple assignments
will be split into multiple different variables, one per assignment.
However, SSA is far from adequate, especially in LLVM bitcode, where
address-taken variables bypass SSA, by being transformed to pointers
whose contents can be changed by multiple store instructions. This is
not specific to analysis of C/C++ or LLVM bitcode. In a Java pointer
analysis, like the one we have briefly presented in
Chapter~\ref{chapter:reflection}, despite SSA, fields of objects can
be, again, updated multiple times.

To take advantage of flow sensitivity and be able to substantially
affect precision, an analysis has to perform \emph{strong updates}. A
store instruction strongly updates the results of an over-approximate
pointer analysis, when it overwrites the previous points-to contents
for the given address. A weak update only \emph{extends} points-to
contents without removing any prior results. However, it is not
generally sound for a flow-sensitive pointer analysis to perform a
strong update without additional reasoning. The reason is that an
abstract object summarizes multiple concrete objects; to strongly
update it, we have to ensure that the update applies to all concrete
objects that it represents.

We previously discussed how the recency-abstraction of
\citeauthor{sas/BalakrishnanR06}~\cite{sas/BalakrishnanR06} allows us
to perform strong updates.
%
Another approach is that of \citeauthor{popl/Lhotak11}, who perform
strong updates only on singleton points-to sets
\cite{popl/Lhotak11}.
%
However, even with strong updates, the cost of flow-sensitivity could
be prohibitive. Improving the efficiency of flow-sensitive analyses
has been studied extensively in the literature
\cite{sigsoft/LiCK11,sigsoft/SuiX16,ecoop/DeD12,iwmm/LiCK13}.
%
\citeauthor*{sas/KhedkerMR12} perform a context-sensitive and
flow-sensitive analysis but limit the points-to information they
compute based on a liveness analysis about pointer ranges
\cite{sas/KhedkerMR12}.
%
\citeauthor*{sas/YeSX14} limit flow-sensitivity by partitioning the
program into regions and maintaining flow-sensitivity only between the
regions but not inside \cite{sas/YeSX14}.
%
Incorporating such approaches and possibly devising other methods that
enable strong updates in limited cases would be a key step in
further improving the precision and overall value of our
structure-sensitive analysis.


\subsection{Integer and String Value Analysis}

Our structure sensitive analysis for C/C++ takes array indices into
account, to introduce a form of array-sensitivity by which it can
statically differentiate distinct array elements. Array indices are not
the only case where our analysis could benefit from integer values. The
number of bytes specified in an allocation instruction (e.g., via the
single parameter of a \code{malloc()} call) could help us filter some
invalid object-type associations.

Such cases, however, are only the tip of the iceberg in how a static
analysis can find use in reasoning about the domains of integer
values. Our analysis could incorporate more sophisticated approaches
in tracking integer-valued quantities and numerical properties of
program variables. For a language like C, which allows pointers to be
treated as integers, even aliasing information could be tied to
the computation of integer value domains. The standard use case of
numeric domains, however, is for answering queries about integer overflows
and array bounds checking.
%
One of the most precise approaches in the computation of numeric
abstract domains is analysis based on the polyhedral domain
\cite{popl/CousotH78}. Cheaper variants include
\begin{inparablank}
\item the Octagons abstract domain~\cite{journals/lisp/Mine06},
\item the Pentagons domain~\cite{sac/LogozzoF08},
\item and difference-bound matrices (or Zones)~\cite{pado/Mine01}.
\end{inparablank}

For similar reasons, we can employ more sophisticated string analyses
to upgrade our reflection handling of
Chapter~\ref{chapter:reflection}, such as that of
\citeauthor*{sas/ChristensenMS03}~\cite{sas/ChristensenMS03}, which we
discussed in the previous chapter.


\subsection{Context Sensitivity}

We have already discussed context sensitivity in the previous section,
and the various strategies in specifying what program elements will
constitute the context.

For the analysis of LLVM bitcode---or any other format that has been
lowered to a C-like language for that purpose, where the OO features
have been translated away to more basic constructs---it is not evident
when and how to use object sensitivity in a generic and parameterized
way, such as the one proposed by \citeauthor*{popl/SmaragdakisBL11}
\cite{popl/SmaragdakisBL11}. Generalizing such approaches to work well
both in the presence and absence of object-oriented features, e.g., by
identifying what parameters could be used as objects, per method call,
is a direction worth investigating.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../thesis"
%%% End:
